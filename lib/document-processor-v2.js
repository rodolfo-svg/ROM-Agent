/**
 * Document Processor V2 - Arquitetura Melhorada
 *
 * FLUXO:
 * 1. LLM Barata (Nova Micro) â†’ Extrai TEXTO COMPLETO do PDF (OCR + estruturaÃ§Ã£o)
 * 2. Salva texto completo no KB como documento intermediÃ¡rio reutilizÃ¡vel
 * 3. LLM Premium (Claude) â†’ LÃª texto completo salvo
 * 4. LLM Premium â†’ Gera mÃºltiplos ficheiros tÃ©cnicos profissionais
 *
 * VANTAGENS:
 * - âœ… ReutilizaÃ§Ã£o: Texto extraÃ­do fica salvo, pode ser analisado mÃºltiplas vezes
 * - âœ… Economia: NÃ£o precisa reprocessar PDF toda vez
 * - âœ… Qualidade: LLM premium trabalha com texto limpo e completo
 * - âœ… Rastreabilidade: Texto intermediÃ¡rio disponÃ­vel para auditoria
 * - âœ… Flexibilidade: Pode gerar diferentes tipos de anÃ¡lise do mesmo texto
 *
 * EXEMPLO:
 * PDF (300 pÃ¡ginas, 1.5M tokens)
 *  â†“
 * Nova Micro extrai: $0.052
 *  â†“
 * Salva: "processo-123_TEXTO_COMPLETO.md"
 *  â†“
 * Claude Sonnet analisa (1.5M tokens): $4.50
 *  â†“
 * Gera: FICHAMENTO.md, ANALISE_JURIDICA.md, CRONOLOGIA.md, RESUMO_EXECUTIVO.md
 *
 * Total: $4.55 (vs $9.00 com abordagem 100% Claude)
 * Economia: 50% + arquivos intermediÃ¡rios salvos!
 */

import fs from 'fs';
import path from 'path';
import { conversar } from '../src/modules/bedrock.js';
import { documentSummarizer } from './document-summarizer.js';
import { ACTIVE_PATHS } from './storage-config.js';
import extractionProgressService from '../src/services/extraction-progress.js';

// Modelos disponÃ­veis (maxTokens = OUTPUT limit)
const MODELS = {
  // LLM Barata (extraÃ§Ã£o)
  'nova-micro': {
    id: 'us.amazon.nova-micro-v1:0',
    name: 'Amazon Nova Micro',
    maxTokens: 5000,  // REAL LIMIT: 5,120 output tokens
    costPer1M: { input: 0.035, output: 0.14 },
    purpose: 'extraction',
    speed: 'very-fast'
  },
  'nova-lite': {
    id: 'us.amazon.nova-lite-v1:0',
    name: 'Amazon Nova Lite',
    maxTokens: 5000,  // REAL LIMIT: 5,120 output tokens
    costPer1M: { input: 0.06, output: 0.24 },
    purpose: 'extraction',
    speed: 'fast'
  },

  // LLM Premium (anÃ¡lise)
  haiku: {
    id: 'us.anthropic.claude-3-5-haiku-20241022-v1:0',
    name: 'Claude 3.5 Haiku',
    maxTokens: 8000,  // REAL LIMIT: 8,192 output tokens
    costPer1M: { input: 1.0, output: 5.0 },
    purpose: 'analysis',
    speed: 'fast'
  },
  sonnet: {
    id: 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',
    name: 'Claude 3.5 Sonnet',
    maxTokens: 8000,  // REAL LIMIT: 8,192 output tokens
    costPer1M: { input: 3.0, output: 15.0 },
    purpose: 'analysis',
    speed: 'medium'
  },
  opus: {
    id: 'us.anthropic.claude-opus-4-20250514-v1:0',
    name: 'Claude Opus 4',
    maxTokens: 16000,  // REAL LIMIT: 16,384 output tokens
    costPer1M: { input: 15.0, output: 75.0 },
    purpose: 'analysis',
    speed: 'slow'
  }
};

export class DocumentProcessorV2 {
  constructor() {
    this.extractedTextCachePath = path.join(ACTIVE_PATHS.data, 'extracted-texts');
    this.ensureDirectories();
  }

  ensureDirectories() {
    if (!fs.existsSync(this.extractedTextCachePath)) {
      fs.mkdirSync(this.extractedTextCachePath, { recursive: true });
    }
  }

  estimateTokens(text) {
    return Math.ceil(text.length / 4);
  }

  /**
   * Gera ID Ãºnico para cache baseado no conteÃºdo
   */
  generateCacheId(documentId, contentHash = null) {
    return `extracted_${documentId}_${contentHash || Date.now()}`;
  }

  /**
   * CHUNKING INTELIGENTE
   *
   * Divide documento grande em chunks menores de forma inteligente,
   * tentando respeitar quebras naturais (parÃ¡grafos, seÃ§Ãµes).
   *
   * @param {string} text - Texto completo a ser dividido
   * @param {number} maxChunkSize - Tamanho mÃ¡ximo de cada chunk em caracteres
   * @returns {Array} Array de chunks com metadados
   */
  smartChunk(text, maxChunkSize = 400000) {
    const chunks = [];
    let currentPosition = 0;
    let chunkNumber = 0;

    while (currentPosition < text.length) {
      let chunkEnd = Math.min(currentPosition + maxChunkSize, text.length);

      // Se nÃ£o Ã© o Ãºltimo chunk, tenta encontrar quebra natural
      if (chunkEnd < text.length) {
        // Procura por quebra de parÃ¡grafo duplo (ideal)
        let breakPoint = text.lastIndexOf('\n\n', chunkEnd);

        // Se nÃ£o encontrou, procura quebra simples
        if (breakPoint <= currentPosition || breakPoint < chunkEnd - 5000) {
          breakPoint = text.lastIndexOf('\n', chunkEnd);
        }

        // Se nÃ£o encontrou, procura ponto final
        if (breakPoint <= currentPosition || breakPoint < chunkEnd - 5000) {
          breakPoint = text.lastIndexOf('.', chunkEnd);
        }

        // Se encontrou quebra natural, usa ela
        if (breakPoint > currentPosition && breakPoint > chunkEnd - 5000) {
          chunkEnd = breakPoint + 1;
        }
      }

      const chunkText = text.substring(currentPosition, chunkEnd);

      chunks.push({
        number: chunkNumber++,
        text: chunkText,
        startPosition: currentPosition,
        endPosition: chunkEnd,
        size: chunkText.length,
        estimatedTokens: this.estimateTokens(chunkText)
      });

      currentPosition = chunkEnd;
    }

    return chunks;
  }

  /**
   * EXTRAÃ‡ÃƒO COM CHUNKING AUTOMÃTICO
   *
   * Processa documentos grandes dividindo em chunks menores,
   * extraindo cada um, e concatenando os resultados.
   *
   * @param {string} rawText - Texto completo do documento
   * @param {string} documentId - ID do documento
   * @param {string} documentName - Nome do documento
   * @param {string|null} jobId - ID do job para rastreamento de progresso
   * @returns {Object} Texto extraÃ­do completo + metadados
   */
  async extractWithChunking(rawText, documentId, documentName, jobId = null) {
    const MAX_CHUNK_SIZE = 400000; // 400k chars = ~100k tokens (seguro para Nova Micro)

    console.log(`\nğŸ“Š [V2 - CHUNKING] DOCUMENTO GRANDE DETECTADO`);
    console.log(`   Tamanho total: ${Math.round(rawText.length / 1000)}k caracteres`);
    console.log(`   EstratÃ©gia: DivisÃ£o em chunks de ${Math.round(MAX_CHUNK_SIZE / 1000)}k caracteres`);

    // Dividir em chunks inteligentes
    const chunks = this.smartChunk(rawText, MAX_CHUNK_SIZE);
    console.log(`   ğŸ“¦ Dividido em ${chunks.length} chunks`);
    console.log(`   â±ï¸  Tempo estimado: ~${chunks.length * 30}s (${Math.round(chunks.length * 30 / 60)} minutos)`);

    // Track progress if jobId provided
    if (jobId) {
      await extractionProgressService.startJob(jobId, 'chunking', chunks.length);
    }

    const extractedParts = [];
    const chunkMetadata = [];
    let totalCost = 0;
    let totalTime = 0;

    // Processar cada chunk
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n   ğŸ”„ [Chunk ${i+1}/${chunks.length}] Processando...`);
      console.log(`      Tamanho: ${Math.round(chunk.size / 1000)}k chars (${chunk.estimatedTokens.toLocaleString()} tokens)`);
      console.log(`      PosiÃ§Ã£o: ${chunk.startPosition} - ${chunk.endPosition}`);

      try {
        const startTime = Date.now();

        // Extrair chunk (sem verificar cache, pois Ã© parte de documento maior)
        const result = await this.extractSingleChunk(
          chunk.text,
          `${documentId}_chunk${i}`,
          `${documentName} - Parte ${i+1}/${chunks.length}`
        );

        const elapsedTime = Math.round((Date.now() - startTime) / 1000);

        console.log(`      âœ… Chunk extraÃ­do em ${elapsedTime}s`);
        console.log(`      ğŸ“Š Output: ${result.extractedText.length.toLocaleString()} chars`);
        console.log(`      ğŸ’° Custo: $${result.metadata.cost.toFixed(4)}`);

        extractedParts.push(result.extractedText);
        totalCost += result.metadata.cost;
        totalTime += elapsedTime;

        chunkMetadata.push({
          chunkNumber: i,
          inputSize: chunk.size,
          outputSize: result.extractedText.length,
          cost: result.metadata.cost,
          time: elapsedTime,
          model: result.metadata.modelUsed
        });

        // Update progress tracking
        if (jobId) {
          await extractionProgressService.updateChunkProgress(jobId, i, {
            inputSize: chunk.size,
            outputSize: result.extractedText.length,
            cost: result.metadata.cost,
            time: elapsedTime,
            model: result.metadata.modelUsed
          });
        }

      } catch (error) {
        console.error(`      âŒ Erro no chunk ${i+1}:`, error.message);

        // Fail job on error
        if (jobId) {
          await extractionProgressService.failJob(jobId, `Erro no chunk ${i+1}: ${error.message}`);
        }

        // Adiciona marcador de erro no texto extraÃ­do
        extractedParts.push(`\n\n[ERRO NA EXTRAÃ‡ÃƒO DO CHUNK ${i+1}: ${error.message}]\n\n`);

        chunkMetadata.push({
          chunkNumber: i,
          error: error.message,
          cost: 0,
          time: 0
        });
      }
    }

    // Concatenar todas as partes
    const fullExtractedText = extractedParts.join('\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n[FIM DA PARTE - CONTINUAÃ‡ÃƒO ABAIXO]\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n');

    console.log(`\n   âœ… CHUNKING CONCLUÃDO`);
    console.log(`   ğŸ“Š Total extraÃ­do: ${Math.round(fullExtractedText.length / 1000)}k caracteres`);
    console.log(`   ğŸ’° Custo total: $${totalCost.toFixed(4)}`);
    console.log(`   â±ï¸  Tempo total: ${totalTime}s (${Math.round(totalTime / 60)} minutos)`);

    // Mark extraction phase as complete if jobId provided
    if (jobId) {
      console.log(`   âœ… Job ${jobId} extraction phase complete`);
    }

    return {
      extractedText: fullExtractedText,
      metadata: {
        method: 'chunking',
        originalSize: rawText.length,
        extractedSize: fullExtractedText.length,
        chunks: chunks.length,
        chunkDetails: chunkMetadata,
        totalCost,
        totalTime,
        documentId,
        documentName,
        extractedAt: new Date().toISOString()
      }
    };
  }

  /**
   * EXTRAÃ‡ÃƒO DE CHUNK INDIVIDUAL
   *
   * Extrai um Ãºnico chunk sem verificar cache (usado internamente por extractWithChunking)
   *
   * @param {string} chunkText - Texto do chunk
   * @param {string} chunkId - ID do chunk
   * @param {string} chunkName - Nome do chunk
   * @returns {Object} Texto extraÃ­do + metadados
   */
  async extractSingleChunk(chunkText, chunkId, chunkName) {
    const startTime = Date.now();

    const extractionPrompt = `
VocÃª Ã© um especialista em extraÃ§Ã£o e estruturaÃ§Ã£o de documentos jurÃ­dicos.

TAREFA:
Extraia e estruture TODO o texto do documento abaixo, corrigindo erros de OCR, organizando parÃ¡grafos, mantendo toda a informaÃ§Ã£o original mas tornando-o limpo e bem formatado.

DIRETRIZES:
1. **Preserve TODA informaÃ§Ã£o**: NÃ£o resuma, nÃ£o omita nada
2. **Corrija erros de OCR**: "rec1amaÃ§Ã£o" â†’ "reclamaÃ§Ã£o"
3. **Mantenha estrutura**: TÃ­tulos, seÃ§Ãµes, numeraÃ§Ãµes
4. **Identifique elementos**: CabeÃ§alhos, rodapÃ©s, assinaturas
5. **Estruture por pÃ¡ginas**: Se houver mÃºltiplas pÃ¡ginas, separe claramente

FORMATO DE SAÃDA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DOCUMENTO EXTRAÃDO E ESTRUTURADO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[CabeÃ§alho do documento, se houver]

[ConteÃºdo da pÃ¡gina 1 limpo e estruturado]

[PÃ¡gina 2]

[ConteÃºdo da pÃ¡gina 2 limpo e estruturado]

...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FIM DO DOCUMENTO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DOCUMENTO BRUTO A EXTRAIR:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${chunkText}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXTRAIA E ESTRUTURE TODO O TEXTO ACIMA:
`;

    let response;
    let modelUsed = 'nova-micro';

    // Tentar com Nova Micro primeiro
    try {
      response = await conversar(extractionPrompt, {
        modelo: MODELS['nova-micro'].id,
        systemPrompt: 'VocÃª Ã© um extrator de texto especializado. Preserve TODA informaÃ§Ã£o, nÃ£o resuma.',
        temperature: 0.1,
        maxTokens: MODELS['nova-micro'].maxTokens,
        enableTools: false,
        enableCache: false
      });

      if (response && response.sucesso === false) {
        throw new Error(`${response.erro} (StatusCode: ${response.statusCode || 'N/A'})`);
      }

    } catch (novaMicroError) {
      // Fallback para Haiku
      console.log(`         âš ï¸  Nova Micro falhou, usando Haiku...`);

      response = await conversar(extractionPrompt, {
        modelo: MODELS['haiku'].id,
        systemPrompt: 'VocÃª Ã© um extrator de texto especializado. Preserve TODA informaÃ§Ã£o, nÃ£o resuma.',
        temperature: 0.1,
        maxTokens: MODELS['haiku'].maxTokens,
        enableTools: false,
        enableCache: false
      });

      if (response && response.sucesso === false) {
        throw new Error(`Both Nova Micro and Haiku failed. Last error: ${response.erro}`);
      }

      modelUsed = 'haiku';
    }

    // Validar resposta
    if (!response || !response.resposta) {
      throw new Error('Resposta do Bedrock invÃ¡lida');
    }

    const elapsedTime = Math.round((Date.now() - startTime) / 1000);
    const inputTokens = this.estimateTokens(chunkText + extractionPrompt);
    const outputTokens = this.estimateTokens(response.resposta);
    const cost = (inputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.input +
                 (outputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.output;

    return {
      extractedText: response.resposta,
      metadata: {
        modelUsed,
        inputTokens,
        outputTokens,
        cost,
        processingTime: elapsedTime
      }
    };
  }

  /**
   * ETAPA 1: ExtraÃ§Ã£o de texto completo com LLM barata
   *
   * @param {string} rawText - Texto bruto do PDF (pode ter erros de OCR, mÃ¡ formataÃ§Ã£o)
   * @param {string} documentId - ID do documento original
   * @param {string} documentName - Nome do documento original
   * @param {string|null} jobId - ID do job para rastreamento de progresso
   * @returns {Object} Texto extraÃ­do e limpo + metadados
   */
  async extractFullText(rawText, documentId, documentName, jobId = null) {
    console.log(`\nğŸ” [V2 - ETAPA 1] EXTRAÃ‡ÃƒO DE TEXTO COMPLETO`);
    console.log(`   Documento: ${documentName}`);
    console.log(`   Tamanho bruto: ${Math.round(rawText.length / 1000)}k caracteres`);

    // DETECÃ‡ÃƒO AUTOMÃTICA DE CHUNKING
    const CHUNKING_THRESHOLD = 400000; // 400k chars = limite seguro para single-pass

    if (rawText.length > CHUNKING_THRESHOLD) {
      console.log(`   âš¡ Documento grande (>${Math.round(CHUNKING_THRESHOLD / 1000)}k chars)`);
      console.log(`   ğŸ”€ Usando estratÃ©gia de CHUNKING automÃ¡tico...`);
      return await this.extractWithChunking(rawText, documentId, documentName, jobId);
    }

    console.log(`   âœ… Documento pequeno (<=${Math.round(CHUNKING_THRESHOLD / 1000)}k chars)`);
    console.log(`   ğŸ“„ Usando extraÃ§Ã£o SINGLE-PASS...`);
    console.log(`   Modelo: ${MODELS['nova-micro'].name}`);

    const startTime = Date.now();

    // Start job tracking for single-pass extraction
    if (jobId) {
      await extractionProgressService.startJob(jobId, 'single-pass', 1);
    }

    // Verifica se jÃ¡ existe extraÃ§Ã£o em cache
    const cacheId = this.generateCacheId(documentId);
    const cachePath = path.join(this.extractedTextCachePath, `${cacheId}.json`);

    if (fs.existsSync(cachePath)) {
      console.log(`   â™»ï¸  Cache encontrado! Lendo extraÃ§Ã£o anterior...`);
      const cached = JSON.parse(fs.readFileSync(cachePath, 'utf-8'));
      console.log(`   âœ… ExtraÃ§Ã£o carregada do cache (economia de tempo e custo)`);
      return cached;
    }

    // Prompt para extraÃ§Ã£o estruturada
    const extractionPrompt = `
VocÃª Ã© um especialista em extraÃ§Ã£o e estruturaÃ§Ã£o de documentos jurÃ­dicos.

TAREFA:
Extraia e estruture TODO o texto do documento abaixo, corrigindo erros de OCR, organizando parÃ¡grafos, mantendo toda a informaÃ§Ã£o original mas tornando-o limpo e bem formatado.

DIRETRIZES:
1. **Preserve TODA informaÃ§Ã£o**: NÃ£o resuma, nÃ£o omita nada
2. **Corrija erros de OCR**: "rec1amaÃ§Ã£o" â†’ "reclamaÃ§Ã£o"
3. **Mantenha estrutura**: TÃ­tulos, seÃ§Ãµes, numeraÃ§Ãµes
4. **Identifique elementos**: CabeÃ§alhos, rodapÃ©s, assinaturas
5. **Preserve formataÃ§Ã£o legal**: CitaÃ§Ãµes, dispositivos legais, valores
6. **Numere pÃ¡ginas**: Se possÃ­vel, indique [PÃ¡gina X]
7. **Organize parÃ¡grafos**: Quebre em parÃ¡grafos lÃ³gicos

FORMATO DE SAÃDA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DOCUMENTO EXTRAÃDO E ESTRUTURADO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[CabeÃ§alho do documento, se houver]

[PÃ¡gina 1]

[ConteÃºdo da pÃ¡gina 1 limpo e estruturado]

[PÃ¡gina 2]

[ConteÃºdo da pÃ¡gina 2 limpo e estruturado]

...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FIM DO DOCUMENTO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DOCUMENTO BRUTO A EXTRAIR:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${rawText}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXTRAIA E ESTRUTURE TODO O TEXTO ACIMA:
`;

    try {
      console.log(`   ğŸ”§ Tentando extraÃ§Ã£o com ${MODELS['nova-micro'].name}...`);

      let response;
      let modelUsed = 'nova-micro';

      try {
        console.log(`\n   ğŸ” DEBUG - Detalhes da Chamada ao Bedrock:`);
        console.log(`   ğŸ“ Tamanho do rawText original: ${Math.round(rawText.length / 1000)}k caracteres`);
        console.log(`   ğŸ“ Tamanho do prompt completo: ${Math.round(extractionPrompt.length / 1000)}k caracteres`);
        console.log(`   ğŸ“ Primeiros 500 chars do rawText:`, rawText.substring(0, 500));
        console.log(`   ğŸ“ Primeiros 500 chars do prompt:`, extractionPrompt.substring(0, 500));
        console.log(`   ğŸ¯ Modelo: ${MODELS['nova-micro'].id}`);
        console.log(`   âš™ï¸  maxTokens: ${MODELS['nova-micro'].maxTokens} (5k output limit)`);

        response = await conversar(extractionPrompt, {
          modelo: MODELS['nova-micro'].id,
          systemPrompt: 'VocÃª Ã© um extrator de texto especializado. Preserve TODA informaÃ§Ã£o, nÃ£o resuma.',
          temperature: 0.1,
          maxTokens: MODELS['nova-micro'].maxTokens,  // 5,000 tokens
          enableTools: false,
          enableCache: false
        });

        console.log(`\n   ğŸ“¦ DEBUG - Resposta Recebida do Nova Micro:`);
        console.log(`   âœ… sucesso: ${response?.sucesso}`);
        console.log(`   ğŸ“Š response.erro: ${response?.erro}`);
        console.log(`   ğŸ“Š response.statusCode: ${response?.statusCode}`);
        console.log(`   ğŸ“Š response keys: ${response ? Object.keys(response).join(', ') : 'null'}`);
        if (response?.resposta) {
          console.log(`   ğŸ“Š response.resposta length: ${response.resposta.length} chars`);
          console.log(`   ğŸ“ Primeiros 200 chars da resposta:`, response.resposta.substring(0, 200));
        }

        if (response && response.sucesso === false) {
          console.log(`   âŒ FALHA CONFIRMADA - Nova Micro retornou sucesso:false`);
        }

        // conversar() retorna objeto com sucesso:false ao invÃ©s de throw
        // Precisamos verificar e forÃ§ar throw para ativar fallback
        if (response && response.sucesso === false) {
          throw new Error(`${response.erro} (StatusCode: ${response.statusCode || 'N/A'})`);
        }

      } catch (novaMicroError) {
        console.log(`\n   âš ï¸  Nova Micro FALHOU: ${novaMicroError.message}`);
        console.log(`   ğŸ”„ Tentando fallback com Claude 3.5 Haiku...\n`);

        try {
          console.log(`   ğŸ” DEBUG - Chamada de Fallback (Haiku):`);
          console.log(`   ğŸ¯ Modelo: ${MODELS['haiku'].id}`);
          console.log(`   âš™ï¸  maxTokens: ${MODELS['haiku'].maxTokens} (8k output limit)`);
          console.log(`   ğŸ“ Tamanho do prompt: ${Math.round(extractionPrompt.length / 1000)}k caracteres (mesmo prompt)`);

          // Fallback para Haiku (mais caro mas funciona)
          response = await conversar(extractionPrompt, {
            modelo: MODELS['haiku'].id,
            systemPrompt: 'VocÃª Ã© um extrator de texto especializado. Preserve TODA informaÃ§Ã£o, nÃ£o resuma.',
            temperature: 0.1,
            maxTokens: MODELS['haiku'].maxTokens,  // 8,000 tokens
            enableTools: false,
            enableCache: false
          });

          console.log(`\n   ğŸ“¦ DEBUG - Resposta Recebida do Haiku:`);
          console.log(`   âœ… sucesso: ${response?.sucesso}`);
          console.log(`   ğŸ“Š response.erro: ${response?.erro}`);
          console.log(`   ğŸ“Š response.statusCode: ${response?.statusCode}`);
          console.log(`   ğŸ“Š response keys: ${response ? Object.keys(response).join(', ') : 'null'}`);
          if (response?.resposta) {
            console.log(`   ğŸ“Š response.resposta length: ${response.resposta.length} chars`);
            console.log(`   ğŸ“ Primeiros 200 chars da resposta:`, response.resposta.substring(0, 200));
          }

          if (response && response.sucesso === false) {
            console.log(`   âŒ FALHA CONFIRMADA - Haiku tambÃ©m retornou sucesso:false`);
            throw new Error(`Both Nova Micro and Haiku failed. Last error: ${response.erro}`);
          }

          modelUsed = 'haiku';
          console.log(`   âœ… Fallback para Haiku bem-sucedido`);

        } catch (haikuError) {
          console.error(`   âŒâŒ FALHA TOTAL: Ambos os modelos falharam`);
          console.error(`   Nova Micro: ${novaMicroError.message}`);
          console.error(`   Haiku: ${haikuError.message}`);
          throw haikuError;
        }
      }

      // Validar resposta
      if (!response) {
        throw new Error('Resposta do Bedrock Ã© null ou undefined');
      }

      // Verificar se houve erro no Bedrock
      if (response.sucesso === false) {
        console.error(`   âŒ Erro do Bedrock:`, response);
        throw new Error(`Bedrock error: ${response.erro || 'Unknown error'}. StatusCode: ${response.statusCode || 'N/A'}`);
      }

      if (!response.resposta) {
        console.error(`   âŒ Resposta do Bedrock sem campo 'resposta':`, JSON.stringify(response, null, 2));
        throw new Error(`Campo 'resposta' nÃ£o encontrado. Response keys: ${Object.keys(response).join(', ')}`);
      }

      const extractedText = response.resposta;
      const elapsedTime = Math.round((Date.now() - startTime) / 1000);

      const inputTokens = this.estimateTokens(rawText);
      const outputTokens = this.estimateTokens(extractedText);
      const cost = (inputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.input +
                   (outputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.output;

      console.log(`   âœ… ExtraÃ§Ã£o concluÃ­da em ${elapsedTime}s`);
      console.log(`   ğŸ¤– Modelo usado: ${MODELS[modelUsed].name}`);
      console.log(`   ğŸ“Š Texto extraÃ­do: ${Math.round(extractedText.length / 1000)}k caracteres`);
      console.log(`   ğŸ’° Custo: $${cost.toFixed(4)}`);

      // Update progress for single-pass extraction
      if (jobId) {
        await extractionProgressService.updateChunkProgress(jobId, 0, {
          inputSize: rawText.length,
          outputSize: extractedText.length,
          cost,
          time: elapsedTime,
          model: modelUsed
        });
      }

      const result = {
        extractedText,
        metadata: {
          documentId,
          documentName,
          originalSize: rawText.length,
          extractedSize: extractedText.length,
          extractedAt: new Date().toISOString(),
          model: modelUsed,
          modelName: MODELS[modelUsed].name,
          usedFallback: modelUsed !== 'nova-micro',
          inputTokens,
          outputTokens,
          cost,
          processingTime: elapsedTime
        }
      };

      // Salva em cache
      fs.writeFileSync(cachePath, JSON.stringify(result, null, 2));
      console.log(`   ğŸ’¾ ExtraÃ§Ã£o salva em cache: ${cacheId}.json`);

      return result;

    } catch (error) {
      console.error(`   âŒ Erro na extraÃ§Ã£o:`, error);

      // Fail job on error
      if (jobId) {
        await extractionProgressService.failJob(jobId, error.message);
      }

      throw error;
    }
  }

  /**
   * ETAPA 2: Salvamento no KB como documento intermediÃ¡rio
   *
   * @param {string} extractedText - Texto completo extraÃ­do
   * @param {string} documentId - ID do documento original
   * @param {string} documentName - Nome do documento original
   */
  async saveExtractedTextToKB(extractedText, documentId, documentName) {
    console.log(`\nğŸ’¾ [V2 - ETAPA 2] SALVAMENTO NO KB`);

    const kbPath = path.join(ACTIVE_PATHS.data, 'kb-documents.json');
    let allDocs = [];

    if (fs.existsSync(kbPath)) {
      allDocs = JSON.parse(fs.readFileSync(kbPath, 'utf-8'));
    }

    // Cria documento intermediÃ¡rio
    const intermediateDoc = {
      id: `kb-extracted-${documentId}-${Date.now()}`,
      name: `${documentName} - TEXTO_COMPLETO.md`,
      originalName: documentName,
      type: 'text/markdown',
      size: extractedText.length,
      uploadedAt: new Date().toISOString(),
      textLength: extractedText.length,
      metadata: {
        isExtractedText: true,
        parentDocument: documentId,
        extractionSource: 'nova-micro',
        purpose: 'intermediate-full-text'
      }
    };

    // Salva arquivo
    const textPath = path.join(this.extractedTextCachePath, `${intermediateDoc.id}.md`);
    fs.writeFileSync(textPath, extractedText, 'utf-8');
    intermediateDoc.path = textPath;

    // Adiciona ao KB
    allDocs.push(intermediateDoc);
    fs.writeFileSync(kbPath, JSON.stringify(allDocs, null, 2));

    console.log(`   âœ… Documento intermediÃ¡rio salvo: ${intermediateDoc.name}`);
    console.log(`   ğŸ“Š Tamanho: ${Math.round(extractedText.length / 1000)}k caracteres`);
    console.log(`   ğŸ†” ID: ${intermediateDoc.id}`);

    return intermediateDoc;
  }

  /**
   * Salva ficheiros tÃ©cnicos no KB e atualiza metadata do documento principal
   *
   * @param {Object} technicalFiles - Objeto com ficheiros {FICHAMENTO, ANALISE_JURIDICA, ...}
   * @param {string} documentId - ID do documento principal
   * @param {string} documentName - Nome do documento principal
   * @param {string} intermediateDocId - ID do documento texto completo
   */
  async saveTechnicalFilesToKB(technicalFiles, documentId, documentName, intermediateDocId, userId = null) {
    console.log(`\nğŸ’¾ [V2 - SALVAMENTO FICHEIROS TÃ‰CNICOS NO KB]`);
    console.log(`   ğŸ” userId: ${userId || 'nÃ£o fornecido'}`);

    const kbPath = path.join(ACTIVE_PATHS.data, 'kb-documents.json');
    const kbDocsDir = path.join(ACTIVE_PATHS.data, 'knowledge-base', 'documents');

    // Garante que diretÃ³rio existe
    if (!fs.existsSync(kbDocsDir)) {
      fs.mkdirSync(kbDocsDir, { recursive: true });
    }

    let allDocs = [];
    if (fs.existsSync(kbPath)) {
      allDocs = JSON.parse(fs.readFileSync(kbPath, 'utf-8'));
    }

    const timestamp = Date.now();
    const savedFiles = [];

    // Mapear nomes de ficheiros para ordem/tipo
    const fileMapping = {
      'FICHAMENTO': { order: 1, prefix: '01_FICHAMENTO', extension: '.md', type: 'FICHAMENTO' },
      'ANALISE_JURIDICA': { order: 2, prefix: '02_ANALISE_JURIDICA', extension: '.md', type: 'ANALISE_JURIDICA' },
      'CRONOLOGIA': { order: 3, prefix: '03_CRONOLOGIA', extension: '.md', type: 'CRONOLOGIA' },
      'RESUMO_EXECUTIVO': { order: 4, prefix: '04_RESUMO_EXECUTIVO', extension: '.md', type: 'RESUMO_EXECUTIVO' }
    };

    // Salvar cada ficheiro
    for (const [fileKey, fileContent] of Object.entries(technicalFiles)) {
      const fileInfo = fileMapping[fileKey];
      if (!fileInfo || !fileContent) continue;

      // ID Ãºnico para o ficheiro
      const fileId = `${timestamp}_${documentName.replace(/\.[^/.]+$/, '')}_${fileInfo.prefix}`;
      const fileName = `${fileId}${fileInfo.extension}`;
      const filePath = path.join(kbDocsDir, fileName);

      // Salva conteÃºdo
      fs.writeFileSync(filePath, fileContent, 'utf-8');

      // Cria metadata do ficheiro
      const fileDoc = {
        id: fileId,
        name: `${fileInfo.prefix}${fileInfo.extension}`,
        originalName: documentName,
        type: 'text/markdown',
        size: fileContent.length,
        uploadedAt: new Date().toISOString(),
        path: filePath,
        userId: userId,  // âœ… FIX: Add userId to document metadata
        metadata: {
          isStructuredDocument: true,
          parentDocument: documentId,
          intermediateDocument: intermediateDocId,
          fileType: fileInfo.type,
          order: fileInfo.order,
          generatedBy: 'document-processor-v2',
          analysisModel: 'claude-sonnet'
        }
      };

      // Salva metadata separado
      const metadataPath = path.join(kbDocsDir, `${fileId}.metadata.json`);
      fs.writeFileSync(metadataPath, JSON.stringify(fileDoc, null, 2));

      // Adiciona ao KB
      allDocs.push(fileDoc);
      savedFiles.push({
        name: fileDoc.name,
        path: filePath,
        type: fileInfo.type,
        size: fileContent.length
      });

      console.log(`   âœ… ${fileInfo.prefix}${fileInfo.extension} salvo (${Math.round(fileContent.length / 1000)}k chars)`);
    }

    // Atualiza documento principal com referÃªncias aos ficheiros estruturados
    const mainDocIndex = allDocs.findIndex(d =>
      d.id === documentId ||
      d.metadata?.parentDocument === documentId ||
      d.originalName === documentName
    );

    if (mainDocIndex !== -1) {
      if (!allDocs[mainDocIndex].metadata) {
        allDocs[mainDocIndex].metadata = {};
      }
      allDocs[mainDocIndex].metadata.structuredDocsInKB = savedFiles;
      allDocs[mainDocIndex].metadata.hasStructuredFiles = true;
      allDocs[mainDocIndex].metadata.structuredFilesCount = savedFiles.length;
      console.log(`   âœ… Metadata do documento principal atualizado`);
    }

    // Salva kb-documents.json atualizado
    fs.writeFileSync(kbPath, JSON.stringify(allDocs, null, 2));

    console.log(`\n   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
    console.log(`   âœ… ${savedFiles.length} ficheiros salvos no KB`);
    console.log(`   ğŸ“‚ DiretÃ³rio: knowledge-base/documents/`);
    console.log(`   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);

    return {
      success: true,
      savedFiles,
      count: savedFiles.length
    };
  }

  /**
   * ETAPA 3: AnÃ¡lise profunda com LLM Premium
   *
   * @param {string} extractedText - Texto completo jÃ¡ limpo
   * @param {string} analysisPrompt - Prompt de anÃ¡lise do usuÃ¡rio
   * @param {string} model - Modelo premium a usar (haiku, sonnet, opus)
   * @param {string} systemPrompt - System prompt customizado
   */
  async analyzeWithPremiumLLM(extractedText, analysisPrompt, model = 'sonnet', systemPrompt = '') {
    console.log(`\nğŸ§  [V2 - ETAPA 3] ANÃLISE COM LLM PREMIUM`);
    console.log(`   Modelo: ${MODELS[model].name}`);
    console.log(`   Texto: ${Math.round(extractedText.length / 1000)}k caracteres (~${this.estimateTokens(extractedText).toLocaleString()} tokens)`);

    const startTime = Date.now();

    const fullPrompt = `
${analysisPrompt}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DOCUMENTO COMPLETO (JÃ EXTRAÃDO E ESTRUTURADO):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

${extractedText}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FIM DO DOCUMENTO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FORNEÃ‡A UMA ANÃLISE COMPLETA E PROFUNDA DO DOCUMENTO ACIMA:
`;

    try {
      const response = await conversar(fullPrompt, {
        modelo: MODELS[model].id,
        systemPrompt: systemPrompt || 'VocÃª Ã© um assistente jurÃ­dico especializado em anÃ¡lise profunda de documentos processuais brasileiros.',
        temperature: 0.3,
        maxTokens: MODELS[model].maxTokens,  // Use model-specific output limit
        enableTools: false,
        enableCache: false
      });

      // Validar resposta
      if (!response) {
        throw new Error('Resposta do Bedrock Ã© null ou undefined');
      }

      // Verificar se houve erro no Bedrock
      if (response.sucesso === false) {
        console.error(`   âŒ Erro do Bedrock:`, response);
        throw new Error(`Bedrock error: ${response.erro || 'Unknown error'}. StatusCode: ${response.statusCode || 'N/A'}`);
      }

      if (!response.resposta) {
        console.error(`   âŒ Resposta do Bedrock sem campo 'resposta':`, JSON.stringify(response, null, 2));
        throw new Error(`Campo 'resposta' nÃ£o encontrado. Response keys: ${Object.keys(response).join(', ')}`);
      }

      const elapsedTime = Math.round((Date.now() - startTime) / 1000);

      const inputTokens = this.estimateTokens(extractedText + analysisPrompt);
      const outputTokens = this.estimateTokens(response.resposta);
      const cost = (inputTokens / 1_000_000) * MODELS[model].costPer1M.input +
                   (outputTokens / 1_000_000) * MODELS[model].costPer1M.output;

      console.log(`   âœ… AnÃ¡lise concluÃ­da em ${elapsedTime}s`);
      console.log(`   ğŸ’° Custo: $${cost.toFixed(4)}`);

      return {
        success: true,
        analysis: response.resposta,
        metadata: {
          model,
          inputTokens,
          outputTokens,
          cost,
          processingTime: elapsedTime
        }
      };

    } catch (error) {
      console.error(`   âŒ Erro na anÃ¡lise:`, error);
      return {
        success: false,
        error: error.message
      };
    }
  }

  /**
   * ETAPA 4: GeraÃ§Ã£o de mÃºltiplos ficheiros tÃ©cnicos
   *
   * @param {string} extractedText - Texto completo jÃ¡ limpo
   * @param {string} documentId - ID do documento
   * @param {string} documentName - Nome do documento
   * @param {string} model - Modelo premium a usar
   */
  async generateTechnicalFiles(extractedText, documentId, documentName, model = 'sonnet', progressCallback = null) {
    console.log(`\nğŸ“„ [V2 - ETAPA 4] GERAÃ‡ÃƒO DE FICHEIROS TÃ‰CNICOS`);
    console.log(`   Modelo: ${MODELS[model].name}`);

    const files = {};
    const costs = [];
    const startTime = Date.now();

    // Ficheiro 1: FICHAMENTO ESTRUTURADO (20-40%)
    if (progressCallback) {
      await progressCallback('fichamento', 20, 'Gerando FICHAMENTO.md...');
    }
    console.log(`\n   ğŸ“‹ Gerando FICHAMENTO.md...`);
    const fichamentoPrompt = `
Crie um FICHAMENTO ESTRUTURADO completo do documento processual, seguindo o formato:

# FICHAMENTO - ${documentName}

## 1. IDENTIFICAÃ‡ÃƒO
- NÃºmero do Processo:
- Classe:
- Ã“rgÃ£o Julgador:
- DistribuiÃ§Ã£o:
- Valor da Causa:
- Assunto:

## 2. PARTES
### Polo Ativo:
### Polo Passivo:

## 3. PEDIDOS
[Liste todos os pedidos com numeraÃ§Ã£o]

## 4. CAUSA DE PEDIR
[Fatos e fundamentos]

## 5. FUNDAMENTAÃ‡ÃƒO JURÃDICA
[Dispositivos legais citados]

## 6. JURISPRUDÃŠNCIA INVOCADA
[Precedentes mencionados]

## 7. DOCUMENTOS ANEXOS
[Lista de documentos juntados]

## 8. MOVIMENTAÃ‡ÃƒO PROCESSUAL
[Principais eventos com datas]

## 9. DECISÃ•ES IMPORTANTES
[Despachos, decisÃµes interlocutÃ³rias, sentenÃ§as]

## 10. VALOR ECONÃ”MICO
[Valores envolvidos, custas, honorÃ¡rios]

Seja COMPLETO e DETALHADO.
`;

    const fichamento = await this.analyzeWithPremiumLLM(extractedText, fichamentoPrompt, model, 'VocÃª Ã© um assistente especializado em fichamento de processos judiciais.');

    if (fichamento.success) {
      files.FICHAMENTO = fichamento.analysis;
      costs.push(fichamento.metadata.cost);
      console.log(`   âœ… FICHAMENTO.md gerado ($${fichamento.metadata.cost.toFixed(4)})`);
    }

    // Ficheiro 2: ANÃLISE JURÃDICA TÃ‰CNICA (40-60%)
    if (progressCallback) {
      await progressCallback('analise', 40, 'Gerando ANALISE_JURIDICA.md...');
    }
    console.log(`\n   âš–ï¸ Gerando ANALISE_JURIDICA.md...`);
    const analisePrompt = `
FaÃ§a uma ANÃLISE JURÃDICA TÃ‰CNICA profunda do documento, incluindo:

# ANÃLISE JURÃDICA - ${documentName}

## 1. RESUMO EXECUTIVO
[SÃ­ntese em 3-5 parÃ¡grafos]

## 2. ANÃLISE DA CAUSA DE PEDIR
[AnÃ¡lise crÃ­tica dos fundamentos fÃ¡ticos]

## 3. ANÃLISE DOS PEDIDOS
[Viabilidade jurÃ­dica de cada pedido]

## 4. FUNDAMENTAÃ‡ÃƒO LEGAL
### Dispositivos Citados:
### AdequaÃ§Ã£o da FundamentaÃ§Ã£o:
### LegislaÃ§Ã£o AplicÃ¡vel NÃ£o Citada:

## 5. JURISPRUDÃŠNCIA
### Precedentes Citados:
### AnÃ¡lise dos Precedentes:
### SugestÃµes de JurisprudÃªncia Adicional:

## 6. PONTOS FORTES
[Liste os pontos fortes da argumentaÃ§Ã£o]

## 7. PONTOS FRACOS / VULNERABILIDADES
[Identifique fragilidades argumentativas]

## 8. ESTRATÃ‰GIA PROCESSUAL
[Avalie a estratÃ©gia adotada]

## 9. RISCOS E OPORTUNIDADES
### Riscos:
### Oportunidades:

## 10. RECOMENDAÃ‡Ã•ES
[SugestÃµes estratÃ©gicas]

Seja CRÃTICO, TÃ‰CNICO e FUNDAMENTADO.
`;

    const analise = await this.analyzeWithPremiumLLM(extractedText, analisePrompt, model, 'VocÃª Ã© um advogado sÃªnior especializado em anÃ¡lise crÃ­tica de peÃ§as processuais.');

    if (analise.success) {
      files.ANALISE_JURIDICA = analise.analysis;
      costs.push(analise.metadata.cost);
      console.log(`   âœ… ANALISE_JURIDICA.md gerado ($${analise.metadata.cost.toFixed(4)})`);
    }

    // Ficheiro 3: CRONOLOGIA DETALHADA (60-75%)
    if (progressCallback) {
      await progressCallback('cronologia', 60, 'Gerando CRONOLOGIA.md...');
    }
    console.log(`\n   ğŸ“… Gerando CRONOLOGIA.md...`);
    const cronologiaPrompt = `
Crie uma LINHA DO TEMPO COMPLETA do processo, extraindo TODAS as datas e eventos:

# CRONOLOGIA - ${documentName}

| Data | Evento | ResponsÃ¡vel | ObservaÃ§Ãµes |
|------|--------|-------------|-------------|
| DD/MM/AAAA | [Evento] | [Quem] | [Detalhes] |

ApÃ³s a tabela, forneÃ§a:

## ANÃLISE TEMPORAL

### Prazos Cumpridos:
### Prazos Descumpridos:
### Eventos CrÃ­ticos:
### PerÃ­odos de InÃ©rcia:
### DuraÃ§Ã£o Total:

Seja EXAUSTIVO - extraia TODAS as datas mencionadas.
`;

    const cronologia = await this.analyzeWithPremiumLLM(extractedText, cronologiaPrompt, model, 'VocÃª Ã© um assistente especializado em anÃ¡lise temporal de processos.');

    if (cronologia.success) {
      files.CRONOLOGIA = cronologia.analysis;
      costs.push(cronologia.metadata.cost);
      console.log(`   âœ… CRONOLOGIA.md gerado ($${cronologia.metadata.cost.toFixed(4)})`);
    }

    // Ficheiro 4: RESUMO EXECUTIVO (75-90%)
    if (progressCallback) {
      await progressCallback('resumo', 75, 'Gerando RESUMO_EXECUTIVO.md...');
    }
    console.log(`\n   ğŸ“ Gerando RESUMO_EXECUTIVO.md...`);
    const resumoPrompt = `
Crie um RESUMO EXECUTIVO sintÃ©tico para leitura rÃ¡pida por tomadores de decisÃ£o:

# RESUMO EXECUTIVO - ${documentName}

## âš–ï¸ NATUREZA
[1-2 frases sobre o tipo de aÃ§Ã£o]

## ğŸ‘¥ PARTES
**Autor:** [Nome]
**RÃ©u:** [Nome]

## ğŸ’° VALOR
R$ [valor] ([extenso])

## ğŸ“‹ PEDIDOS PRINCIPAIS
1. [Pedido 1]
2. [Pedido 2]
3. [Pedido 3]

## ğŸ¯ CAUSA DE PEDIR (Resumo)
[2-3 parÃ¡grafos sintÃ©ticos]

## âš–ï¸ FUNDAMENTAÃ‡ÃƒO JURÃDICA
- [Lei X, art. Y]
- [Lei Z, art. W]

## ğŸ“Š STATUS ATUAL
[Fase processual e Ãºltima movimentaÃ§Ã£o]

## âš ï¸ PONTOS DE ATENÃ‡ÃƒO
- [Ponto crÃ­tico 1]
- [Ponto crÃ­tico 2]

## ğŸ“ˆ PROGNÃ“STICO
[AvaliaÃ§Ã£o sintÃ©tica de chances de Ãªxito]

---
**Gerado em:** [Data]
**Analista:** ROM Agent (IA)

MÃ¡ximo 2 pÃ¡ginas. Seja SINTÃ‰TICO e OBJETIVO.
`;

    const resumo = await this.analyzeWithPremiumLLM(extractedText, resumoPrompt, model, 'VocÃª Ã© um analista que cria resumos executivos para advogados sÃªniores.');

    if (resumo.success) {
      files.RESUMO_EXECUTIVO = resumo.analysis;
      costs.push(resumo.metadata.cost);
      console.log(`   âœ… RESUMO_EXECUTIVO.md gerado ($${resumo.metadata.cost.toFixed(4)})`);
    }

    const totalTime = Math.round((Date.now() - startTime) / 1000);
    const totalCost = costs.reduce((sum, c) => sum + c, 0);

    console.log(`\n   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
    console.log(`   âœ… ${Object.keys(files).length} ficheiros gerados`);
    console.log(`   â±ï¸ Tempo total: ${totalTime}s`);
    console.log(`   ğŸ’° Custo total: $${totalCost.toFixed(4)}`);
    console.log(`   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);

    return {
      success: true,
      files,
      metadata: {
        filesGenerated: Object.keys(files).length,
        totalCost,
        totalTime
      }
    };
  }

  /**
   * MÃ‰TODO PRINCIPAL: Processa documento completo (todas as 4 etapas)
   *
   * @param {string} rawText - Texto bruto do PDF
   * @param {string} documentId - ID do documento
   * @param {string} documentName - Nome do documento
   * @param {Object} options - OpÃ§Ãµes de processamento
   */
  async processComplete(rawText, documentId, documentName, options = {}) {
    // Helper para logar memÃ³ria
    const logMemory = (stage) => {
      const used = process.memoryUsage();
      console.log(`   ğŸ’¾ [${stage}] MemÃ³ria: RSS=${Math.round(used.rss/1024/1024)}MB, Heap=${Math.round(used.heapUsed/1024/1024)}MB/${Math.round(used.heapTotal/1024/1024)}MB`);
    };

    console.log(`\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—`);
    console.log(`â•‘  ğŸ“„ DOCUMENT PROCESSOR V2 - ARQUITETURA MELHORADA           â•‘`);
    console.log(`â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
    console.log(`\nğŸ“„ Documento: ${documentName}`);
    console.log(`ğŸ“Š Tamanho: ${Math.round(rawText.length / 1000)}k caracteres (~${this.estimateTokens(rawText).toLocaleString()} tokens)`);
    logMemory('INÃCIO');

    const {
      extractionModel = 'nova-micro',
      analysisModel = 'sonnet',
      generateFiles = true,
      saveToKB = true,
      userId = null,  // âœ… FIX: Extract userId from options
      progressCallback = null
    } = options;

    const totalStartTime = Date.now();
    const costs = [];

    try {
      // ETAPA 1: ExtraÃ§Ã£o (0-15%)
      if (progressCallback) {
        await progressCallback('extraction', 0, 'Extraindo texto com Nova Micro...');
      }

      console.log(`\nğŸ” [ETAPA 1] Iniciando extraÃ§Ã£o de texto...`);
      logMemory('PRÃ‰-EXTRAÃ‡ÃƒO');

      const extraction = await this.extractFullText(rawText, documentId, documentName);
      costs.push(extraction.metadata.cost);

      logMemory('PÃ“S-EXTRAÃ‡ÃƒO');
      console.log(`   âœ… ExtraÃ§Ã£o completa: ${Math.round(extraction.extractedText.length/1000)}k chars`);

      // ETAPA 2: Salvamento no KB (15-20%)
      if (progressCallback) {
        await progressCallback('saving', 15, 'Salvando texto extraÃ­do no KB...');
      }

      console.log(`\nğŸ’¾ [ETAPA 2] Salvando texto extraÃ­do no KB...`);
      logMemory('PRÃ‰-SALVAMENTO');

      let intermediateDoc = null;
      if (saveToKB) {
        intermediateDoc = await this.saveExtractedTextToKB(
          extraction.extractedText,
          documentId,
          documentName
        );
        console.log(`   âœ… Texto salvo no KB: ${intermediateDoc.id}`);
      }

      logMemory('PÃ“S-SALVAMENTO');

      // ETAPA 3-6: GeraÃ§Ã£o de ficheiros tÃ©cnicos (20-90%)
      let technicalFiles = null;
      let savedFilesResult = null;
      if (generateFiles) {
        console.log(`\nğŸ“ [ETAPA 3-6] Gerando ficheiros tÃ©cnicos com IA...`);
        logMemory('PRÃ‰-GERAÃ‡ÃƒO-FICHEIROS');

        if (progressCallback) {
          await progressCallback('fichamento', 20, 'Gerando FICHAMENTO.md...');
        }

        technicalFiles = await this.generateTechnicalFiles(
          extraction.extractedText,
          documentId,
          documentName,
          analysisModel,
          progressCallback
        );
        costs.push(technicalFiles.metadata.totalCost);

        // ETAPA 7: Salvar ficheiros tÃ©cnicos no KB (90-100%)
        if (progressCallback) {
          await progressCallback('saving_files', 90, 'Salvando ficheiros estruturados no KB...');
        }

        if (saveToKB && technicalFiles.success && technicalFiles.files) {
          savedFilesResult = await this.saveTechnicalFilesToKB(
            technicalFiles.files,
            documentId,
            documentName,
            intermediateDoc?.id || documentId,
            userId  // âœ… FIX: Pass userId to saveTechnicalFilesToKB
          );
        }
      }

      const totalTime = Math.round((Date.now() - totalStartTime) / 1000);
      const totalCost = costs.reduce((sum, c) => sum + c, 0);

      console.log(`\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—`);
      console.log(`â•‘  âœ… PROCESSAMENTO COMPLETO CONCLUÃDO                         â•‘`);
      console.log(`â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
      console.log(`\nâ±ï¸  Tempo total: ${totalTime}s`);
      console.log(`ğŸ’° Custo total: $${totalCost.toFixed(4)}`);
      console.log(`\nğŸ“¦ Resultados:`);
      console.log(`   1. Texto completo extraÃ­do e salvo no KB`);
      if (technicalFiles) {
        console.log(`   2. ${technicalFiles.metadata.filesGenerated} ficheiros tÃ©cnicos gerados`);
      }
      if (savedFilesResult && savedFilesResult.success) {
        console.log(`   3. ${savedFilesResult.count} ficheiros salvos no KB (disponÃ­veis para chat)`);
      }
      console.log(`\nğŸ’¡ Vantagens:`);
      console.log(`   âœ… Texto extraÃ­do reutilizÃ¡vel (cache)`);
      console.log(`   âœ… Economia vs abordagem 100% Claude: ~50%`);
      console.log(`   âœ… Ficheiros profissionais prontos para uso`);
      console.log(`   âœ… KB Loader carrega automaticamente no chat`);

      return {
        success: true,
        extraction: extraction.metadata,
        intermediateDoc,
        technicalFiles: technicalFiles?.files || null,
        savedFiles: savedFilesResult?.savedFiles || [],
        metadata: {
          totalTime,
          totalCost,
          extractionCost: extraction.metadata.cost,
          analysisCost: technicalFiles?.metadata.totalCost || 0,
          filesGenerated: technicalFiles?.metadata.filesGenerated || 0,
          filesSavedToKB: savedFilesResult?.count || 0
        }
      };

    } catch (error) {
      console.error(`\nâŒ Erro no processamento V2:`, error);
      return {
        success: false,
        error: error.message
      };
    }
  }
}

// Singleton
export const documentProcessorV2 = new DocumentProcessorV2();
