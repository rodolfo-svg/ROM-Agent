/**
 * Document Processor V2 - Arquitetura Melhorada
 *
 * FLUXO:
 * 1. LLM Barata (Nova Micro) ‚Üí Extrai TEXTO COMPLETO do PDF (OCR + estrutura√ß√£o)
 * 2. Salva texto completo no KB como documento intermedi√°rio reutiliz√°vel
 * 3. LLM Premium (Claude) ‚Üí L√™ texto completo salvo
 * 4. LLM Premium ‚Üí Gera m√∫ltiplos ficheiros t√©cnicos profissionais
 *
 * VANTAGENS:
 * - ‚úÖ Reutiliza√ß√£o: Texto extra√≠do fica salvo, pode ser analisado m√∫ltiplas vezes
 * - ‚úÖ Economia: N√£o precisa reprocessar PDF toda vez
 * - ‚úÖ Qualidade: LLM premium trabalha com texto limpo e completo
 * - ‚úÖ Rastreabilidade: Texto intermedi√°rio dispon√≠vel para auditoria
 * - ‚úÖ Flexibilidade: Pode gerar diferentes tipos de an√°lise do mesmo texto
 *
 * EXEMPLO:
 * PDF (300 p√°ginas, 1.5M tokens)
 *  ‚Üì
 * Nova Micro extrai: $0.052
 *  ‚Üì
 * Salva: "processo-123_TEXTO_COMPLETO.md"
 *  ‚Üì
 * Claude Sonnet analisa (1.5M tokens): $4.50
 *  ‚Üì
 * Gera: FICHAMENTO.md, ANALISE_JURIDICA.md, CRONOLOGIA.md, RESUMO_EXECUTIVO.md
 *
 * Total: $4.55 (vs $9.00 com abordagem 100% Claude)
 * Economia: 50% + arquivos intermedi√°rios salvos!
 */

import fs from 'fs';
import path from 'path';
import { conversar } from '../src/modules/bedrock.js';
import { documentSummarizer } from './document-summarizer.js';
import { ACTIVE_PATHS } from './storage-config.js';
import extractionProgressService from '../src/services/extraction-progress.js';
import BATCH_ANALYSIS_PROMPT from './batch-analysis-prompt.js';
import { extractionPersistenceManager } from './extraction-persistence.js';

// Modelos dispon√≠veis (maxTokens = OUTPUT limit)
const MODELS = {
  // LLM Barata (extra√ß√£o)
  'nova-micro': {
    id: 'us.amazon.nova-micro-v1:0',
    name: 'Amazon Nova Micro',
    maxTokens: 5000,  // REAL LIMIT: 5,120 output tokens
    costPer1M: { input: 0.035, output: 0.14 },
    purpose: 'extraction',
    speed: 'very-fast'
  },
  'nova-lite': {
    id: 'us.amazon.nova-lite-v1:0',
    name: 'Amazon Nova Lite',
    maxTokens: 5000,  // REAL LIMIT: 5,120 output tokens
    costPer1M: { input: 0.06, output: 0.24 },
    purpose: 'extraction',
    speed: 'fast'
  },

  // LLM Premium (an√°lise)
  haiku: {
    id: 'us.anthropic.claude-3-5-haiku-20241022-v1:0',
    name: 'Claude 3.5 Haiku',
    maxTokens: 8000,  // REAL LIMIT: 8,192 output tokens
    costPer1M: { input: 1.0, output: 5.0 },
    purpose: 'analysis',
    speed: 'fast'
  },
  sonnet: {
    id: 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',
    name: 'Claude 3.5 Sonnet',
    maxTokens: 8000,  // REAL LIMIT: 8,192 output tokens
    costPer1M: { input: 3.0, output: 15.0 },
    purpose: 'analysis',
    speed: 'medium'
  },
  opus: {
    id: 'us.anthropic.claude-opus-4-20250514-v1:0',
    name: 'Claude Opus 4',
    maxTokens: 16000,  // REAL LIMIT: 16,384 output tokens
    costPer1M: { input: 15.0, output: 75.0 },
    purpose: 'analysis',
    speed: 'slow'
  }
};

export class DocumentProcessorV2 {
  constructor() {
    this.extractedTextCachePath = path.join(ACTIVE_PATHS.data, 'extracted-texts');
    this.ensureDirectories();
  }

  ensureDirectories() {
    if (!fs.existsSync(this.extractedTextCachePath)) {
      fs.mkdirSync(this.extractedTextCachePath, { recursive: true });
    }
  }

  estimateTokens(text) {
    return Math.ceil(text.length / 4);
  }

  /**
   * Gera ID √∫nico para cache baseado no conte√∫do
   */
  generateCacheId(documentId, contentHash = null) {
    return `extracted_${documentId}_${contentHash || Date.now()}`;
  }

  /**
   * CHUNKING INTELIGENTE
   *
   * Divide documento grande em chunks menores de forma inteligente,
   * tentando respeitar quebras naturais (par√°grafos, se√ß√µes).
   *
   * @param {string} text - Texto completo a ser dividido
   * @param {number} maxChunkSize - Tamanho m√°ximo de cada chunk em caracteres
   * @returns {Array} Array de chunks com metadados
   */
  smartChunk(text, maxChunkSize = 400000) {
    const chunks = [];
    let currentPosition = 0;
    let chunkNumber = 0;

    while (currentPosition < text.length) {
      let chunkEnd = Math.min(currentPosition + maxChunkSize, text.length);

      // Se n√£o √© o √∫ltimo chunk, tenta encontrar quebra natural
      if (chunkEnd < text.length) {
        // Procura por quebra de par√°grafo duplo (ideal)
        let breakPoint = text.lastIndexOf('\n\n', chunkEnd);

        // Se n√£o encontrou, procura quebra simples
        if (breakPoint <= currentPosition || breakPoint < chunkEnd - 5000) {
          breakPoint = text.lastIndexOf('\n', chunkEnd);
        }

        // Se n√£o encontrou, procura ponto final
        if (breakPoint <= currentPosition || breakPoint < chunkEnd - 5000) {
          breakPoint = text.lastIndexOf('.', chunkEnd);
        }

        // Se encontrou quebra natural, usa ela
        if (breakPoint > currentPosition && breakPoint > chunkEnd - 5000) {
          chunkEnd = breakPoint + 1;
        }
      }

      const chunkText = text.substring(currentPosition, chunkEnd);

      chunks.push({
        number: chunkNumber++,
        text: chunkText,
        startPosition: currentPosition,
        endPosition: chunkEnd,
        size: chunkText.length,
        estimatedTokens: this.estimateTokens(chunkText)
      });

      currentPosition = chunkEnd;
    }

    return chunks;
  }

  /**
   * EXTRA√á√ÉO COM CHUNKING AUTOM√ÅTICO
   *
   * Processa documentos grandes dividindo em chunks menores,
   * extraindo cada um, e concatenando os resultados.
   *
   * @param {string} rawText - Texto completo do documento
   * @param {string} documentId - ID do documento
   * @param {string} documentName - Nome do documento
   * @param {string|null} jobId - ID do job para rastreamento de progresso
   * @returns {Object} Texto extra√≠do completo + metadados
   */
  async extractWithChunking(rawText, documentId, documentName, jobId = null) {
    const MAX_CHUNK_SIZE = 400000; // 400k chars = ~100k tokens (seguro para Nova Micro)

    console.log(`\nüìä [V2 - CHUNKING] DOCUMENTO GRANDE DETECTADO`);
    console.log(`   Tamanho total: ${Math.round(rawText.length / 1000)}k caracteres`);
    console.log(`   Estrat√©gia: Divis√£o em chunks de ${Math.round(MAX_CHUNK_SIZE / 1000)}k caracteres`);

    // Dividir em chunks inteligentes
    const chunks = this.smartChunk(rawText, MAX_CHUNK_SIZE);
    console.log(`   üì¶ Dividido em ${chunks.length} chunks`);
    console.log(`   ‚è±Ô∏è  Tempo estimado: ~${chunks.length * 30}s (${Math.round(chunks.length * 30 / 60)} minutos)`);

    // Track progress if jobId provided
    if (jobId) {
      await extractionProgressService.startJob(jobId, 'chunking', chunks.length);
    }

    const extractedParts = new Array(chunks.length); // Pr√©-alocar array para manter ordem
    const chunkMetadata = [];
    let totalCost = 0;
    let totalTime = 0;

    // üöÄ OTIMIZA√á√ÉO: Processar chunks em PARALELO (3 ao mesmo tempo)
    const PARALLEL_CHUNKS = 3;
    const overallStartTime = Date.now();

    console.log(`   üöÄ Processando ${PARALLEL_CHUNKS} chunks em paralelo para acelerar...`);

    // Processar chunks em batches paralelos
    for (let batchStart = 0; batchStart < chunks.length; batchStart += PARALLEL_CHUNKS) {
      const batchEnd = Math.min(batchStart + PARALLEL_CHUNKS, chunks.length);
      const batch = chunks.slice(batchStart, batchEnd);

      console.log(`\n   üì¶ Batch ${Math.floor(batchStart/PARALLEL_CHUNKS)+1}/${Math.ceil(chunks.length/PARALLEL_CHUNKS)}: Processando chunks ${batchStart+1}-${batchEnd}...`);

      // Processar batch em paralelo
      const batchPromises = batch.map(async (chunk, batchIndex) => {
        const i = batchStart + batchIndex;
        console.log(`\n   üîÑ [Chunk ${i+1}/${chunks.length}] Processando...`);
        console.log(`      Tamanho: ${Math.round(chunk.size / 1000)}k chars (${chunk.estimatedTokens.toLocaleString()} tokens)`);

        try {
          const startTime = Date.now();

          const result = await this.extractSingleChunk(
            chunk.text,
            `${documentId}_chunk${i}`,
            `${documentName} - Parte ${i+1}/${chunks.length}`
          );

          const elapsedTime = Math.round((Date.now() - startTime) / 1000);

          console.log(`      ‚úÖ Chunk ${i+1} extra√≠do em ${elapsedTime}s`);
          console.log(`      üìä Output: ${result.extractedText.length.toLocaleString()} chars`);
          console.log(`      üí∞ Custo: $${result.metadata.cost.toFixed(4)}`);

          return {
            index: i,
            text: result.extractedText,
            metadata: {
              chunkNumber: i,
              inputSize: chunk.size,
              outputSize: result.extractedText.length,
              cost: result.metadata.cost,
              time: elapsedTime,
              model: result.metadata.modelUsed
            }
          };

        } catch (error) {
          console.error(`      ‚ùå Erro no chunk ${i+1}:`, error.message);

          return {
            index: i,
            text: `\n\n[ERRO NA EXTRA√á√ÉO DO CHUNK ${i+1}: ${error.message}]\n\n`,
            metadata: {
              chunkNumber: i,
              error: error.message,
              cost: 0,
              time: 0
            }
          };
        }
      });

      // Aguardar batch completar
      const batchResults = await Promise.all(batchPromises);

      // Adicionar resultados ao array na ordem correta
      for (const result of batchResults) {
        extractedParts[result.index] = result.text;
        chunkMetadata.push(result.metadata);
        totalCost += result.metadata.cost || 0;
        totalTime += result.metadata.time || 0;

        // Update progress tracking
        if (jobId) {
          await extractionProgressService.updateChunkProgress(jobId, result.index, result.metadata);
        }
      }

      console.log(`   ‚úÖ Batch ${Math.floor(batchStart/PARALLEL_CHUNKS)+1} conclu√≠do`);
    }

    const overallTime = Math.round((Date.now() - overallStartTime) / 1000);

    // Concatenar todas as partes (filter para remover undefined se houver)
    const fullExtractedText = extractedParts.filter(p => p).join('\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n[FIM DA PARTE - CONTINUA√á√ÉO ABAIXO]\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n');

    console.log(`\n   ‚úÖ CHUNKING CONCLU√çDO`);
    console.log(`   üìä Total extra√≠do: ${Math.round(fullExtractedText.length / 1000)}k caracteres`);
    console.log(`   üí∞ Custo total: $${totalCost.toFixed(4)}`);
    console.log(`   ‚è±Ô∏è  Tempo real: ${overallTime}s (${Math.round(overallTime / 60)} minutos)`);
    console.log(`   ‚ö° Speedup: ${Math.round((totalTime / overallTime) * 10) / 10}x vs sequencial`);

    // Mark extraction phase as complete if jobId provided
    if (jobId) {
      console.log(`   ‚úÖ Job ${jobId} extraction phase complete`);
    }

    return {
      extractedText: fullExtractedText,
      metadata: {
        method: 'chunking-parallel',
        originalSize: rawText.length,
        extractedSize: fullExtractedText.length,
        chunks: chunks.length,
        parallelBatchSize: PARALLEL_CHUNKS,
        chunkDetails: chunkMetadata,
        cost: totalCost,
        processingTime: overallTime, // Tempo real (paralelo)
        sequentialTime: totalTime, // Tempo que levaria sequencial
        speedup: Math.round((totalTime / overallTime) * 10) / 10,
        documentId,
        documentName,
        extractedAt: new Date().toISOString()
      }
    };
  }

  /**
   * EXTRA√á√ÉO DE CHUNK INDIVIDUAL
   *
   * Extrai um √∫nico chunk sem verificar cache (usado internamente por extractWithChunking)
   *
   * @param {string} chunkText - Texto do chunk
   * @param {string} chunkId - ID do chunk
   * @param {string} chunkName - Nome do chunk
   * @returns {Object} Texto extra√≠do + metadados
   */
  async extractSingleChunk(chunkText, chunkId, chunkName) {
    const startTime = Date.now();

    const extractionPrompt = `
Voc√™ √© um especialista em extra√ß√£o e estrutura√ß√£o de documentos jur√≠dicos.

TAREFA:
Extraia e estruture TODO o texto do documento abaixo, corrigindo erros de OCR, organizando par√°grafos, mantendo toda a informa√ß√£o original mas tornando-o limpo e bem formatado.

DIRETRIZES:
1. **Preserve TODA informa√ß√£o**: N√£o resuma, n√£o omita nada
2. **Corrija erros de OCR**: "rec1ama√ß√£o" ‚Üí "reclama√ß√£o"
3. **Mantenha estrutura**: T√≠tulos, se√ß√µes, numera√ß√µes
4. **Identifique elementos**: Cabe√ßalhos, rodap√©s, assinaturas
5. **Estruture por p√°ginas**: Se houver m√∫ltiplas p√°ginas, separe claramente

FORMATO DE SA√çDA:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
DOCUMENTO EXTRA√çDO E ESTRUTURADO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

[Cabe√ßalho do documento, se houver]

[Conte√∫do da p√°gina 1 limpo e estruturado]

[P√°gina 2]

[Conte√∫do da p√°gina 2 limpo e estruturado]

...

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
FIM DO DOCUMENTO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DOCUMENTO BRUTO A EXTRAIR:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
${chunkText}
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

EXTRAIA E ESTRUTURE TODO O TEXTO ACIMA:
`;

    let response;
    let modelUsed = 'nova-micro';

    // Tentar com Nova Micro primeiro
    try {
      response = await conversar(extractionPrompt, {
        modelo: MODELS['nova-micro'].id,
        systemPrompt: 'Voc√™ √© um extrator de texto especializado. Preserve TODA informa√ß√£o, n√£o resuma.',
        temperature: 0.1,
        maxTokens: MODELS['nova-micro'].maxTokens,
        enableTools: false,
        enableCache: false
      });

      if (response && response.sucesso === false) {
        throw new Error(`${response.erro} (StatusCode: ${response.statusCode || 'N/A'})`);
      }

    } catch (novaMicroError) {
      // Fallback para Haiku
      console.log(`         ‚ö†Ô∏è  Nova Micro falhou, usando Haiku...`);

      response = await conversar(extractionPrompt, {
        modelo: MODELS['haiku'].id,
        systemPrompt: 'Voc√™ √© um extrator de texto especializado. Preserve TODA informa√ß√£o, n√£o resuma.',
        temperature: 0.1,
        maxTokens: MODELS['haiku'].maxTokens,
        enableTools: false,
        enableCache: false
      });

      if (response && response.sucesso === false) {
        throw new Error(`Both Nova Micro and Haiku failed. Last error: ${response.erro}`);
      }

      modelUsed = 'haiku';
    }

    // Validar resposta
    if (!response || !response.resposta) {
      throw new Error('Resposta do Bedrock inv√°lida');
    }

    const elapsedTime = Math.round((Date.now() - startTime) / 1000);
    const inputTokens = this.estimateTokens(chunkText + extractionPrompt);
    const outputTokens = this.estimateTokens(response.resposta);
    const cost = (inputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.input +
                 (outputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.output;

    return {
      extractedText: response.resposta,
      metadata: {
        modelUsed,
        inputTokens,
        outputTokens,
        cost,
        processingTime: elapsedTime
      }
    };
  }

  /**
   * ETAPA 1: Extra√ß√£o de texto completo com LLM barata
   *
   * @param {string} rawText - Texto bruto do PDF (pode ter erros de OCR, m√° formata√ß√£o)
   * @param {string} documentId - ID do documento original
   * @param {string} documentName - Nome do documento original
   * @param {string|null} jobId - ID do job para rastreamento de progresso
   * @returns {Object} Texto extra√≠do e limpo + metadados
   */
  async extractFullText(rawText, documentId, documentName, jobId = null) {
    console.log(`\nüîç [V2 - ETAPA 1] EXTRA√á√ÉO DE TEXTO COMPLETO`);
    console.log(`   Documento: ${documentName}`);
    console.log(`   Tamanho bruto: ${Math.round(rawText.length / 1000)}k caracteres`);

    // DETEC√á√ÉO AUTOM√ÅTICA DE CHUNKING
    const CHUNKING_THRESHOLD = 400000; // 400k chars = limite seguro para single-pass

    if (rawText.length > CHUNKING_THRESHOLD) {
      console.log(`   ‚ö° Documento grande (>${Math.round(CHUNKING_THRESHOLD / 1000)}k chars)`);
      console.log(`   üîÄ Usando estrat√©gia de CHUNKING autom√°tico...`);
      return await this.extractWithChunking(rawText, documentId, documentName, jobId);
    }

    console.log(`   ‚úÖ Documento pequeno (<=${Math.round(CHUNKING_THRESHOLD / 1000)}k chars)`);
    console.log(`   üìÑ Usando extra√ß√£o SINGLE-PASS...`);
    console.log(`   Modelo: ${MODELS['nova-micro'].name}`);

    const startTime = Date.now();

    // Start job tracking for single-pass extraction
    if (jobId) {
      await extractionProgressService.startJob(jobId, 'single-pass', 1);
    }

    // Verifica se j√° existe extra√ß√£o em cache
    const cacheId = this.generateCacheId(documentId);
    const cachePath = path.join(this.extractedTextCachePath, `${cacheId}.json`);

    if (fs.existsSync(cachePath)) {
      console.log(`   ‚ôªÔ∏è  Cache encontrado! Lendo extra√ß√£o anterior...`);
      const cached = JSON.parse(fs.readFileSync(cachePath, 'utf-8'));
      console.log(`   ‚úÖ Extra√ß√£o carregada do cache (economia de tempo e custo)`);
      return cached;
    }

    // Prompt para extra√ß√£o estruturada
    const extractionPrompt = `
Voc√™ √© um especialista em extra√ß√£o e estrutura√ß√£o de documentos jur√≠dicos.

TAREFA:
Extraia e estruture TODO o texto do documento abaixo, corrigindo erros de OCR, organizando par√°grafos, mantendo toda a informa√ß√£o original mas tornando-o limpo e bem formatado.

DIRETRIZES:
1. **Preserve TODA informa√ß√£o**: N√£o resuma, n√£o omita nada
2. **Corrija erros de OCR**: "rec1ama√ß√£o" ‚Üí "reclama√ß√£o"
3. **Mantenha estrutura**: T√≠tulos, se√ß√µes, numera√ß√µes
4. **Identifique elementos**: Cabe√ßalhos, rodap√©s, assinaturas
5. **Preserve formata√ß√£o legal**: Cita√ß√µes, dispositivos legais, valores
6. **Numere p√°ginas**: Se poss√≠vel, indique [P√°gina X]
7. **Organize par√°grafos**: Quebre em par√°grafos l√≥gicos

FORMATO DE SA√çDA:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
DOCUMENTO EXTRA√çDO E ESTRUTURADO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

[Cabe√ßalho do documento, se houver]

[P√°gina 1]

[Conte√∫do da p√°gina 1 limpo e estruturado]

[P√°gina 2]

[Conte√∫do da p√°gina 2 limpo e estruturado]

...

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
FIM DO DOCUMENTO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DOCUMENTO BRUTO A EXTRAIR:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
${rawText}
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

EXTRAIA E ESTRUTURE TODO O TEXTO ACIMA:
`;

    try {
      console.log(`   üîß Tentando extra√ß√£o com ${MODELS['nova-micro'].name}...`);

      let response;
      let modelUsed = 'nova-micro';

      try {
        console.log(`\n   üîç DEBUG - Detalhes da Chamada ao Bedrock:`);
        console.log(`   üìè Tamanho do rawText original: ${Math.round(rawText.length / 1000)}k caracteres`);
        console.log(`   üìè Tamanho do prompt completo: ${Math.round(extractionPrompt.length / 1000)}k caracteres`);
        console.log(`   üìù Primeiros 500 chars do rawText:`, rawText.substring(0, 500));
        console.log(`   üìù Primeiros 500 chars do prompt:`, extractionPrompt.substring(0, 500));
        console.log(`   üéØ Modelo: ${MODELS['nova-micro'].id}`);
        console.log(`   ‚öôÔ∏è  maxTokens: ${MODELS['nova-micro'].maxTokens} (5k output limit)`);

        response = await conversar(extractionPrompt, {
          modelo: MODELS['nova-micro'].id,
          systemPrompt: 'Voc√™ √© um extrator de texto especializado. Preserve TODA informa√ß√£o, n√£o resuma.',
          temperature: 0.1,
          maxTokens: MODELS['nova-micro'].maxTokens,  // 5,000 tokens
          enableTools: false,
          enableCache: false
        });

        console.log(`\n   üì¶ DEBUG - Resposta Recebida do Nova Micro:`);
        console.log(`   ‚úÖ sucesso: ${response?.sucesso}`);
        console.log(`   üìä response.erro: ${response?.erro}`);
        console.log(`   üìä response.statusCode: ${response?.statusCode}`);
        console.log(`   üìä response keys: ${response ? Object.keys(response).join(', ') : 'null'}`);
        if (response?.resposta) {
          console.log(`   üìä response.resposta length: ${response.resposta.length} chars`);
          console.log(`   üìù Primeiros 200 chars da resposta:`, response.resposta.substring(0, 200));
        }

        if (response && response.sucesso === false) {
          console.log(`   ‚ùå FALHA CONFIRMADA - Nova Micro retornou sucesso:false`);
        }

        // conversar() retorna objeto com sucesso:false ao inv√©s de throw
        // Precisamos verificar e for√ßar throw para ativar fallback
        if (response && response.sucesso === false) {
          throw new Error(`${response.erro} (StatusCode: ${response.statusCode || 'N/A'})`);
        }

      } catch (novaMicroError) {
        console.log(`\n   ‚ö†Ô∏è  Nova Micro FALHOU: ${novaMicroError.message}`);
        console.log(`   üîÑ Tentando fallback com Claude 3.5 Haiku...\n`);

        try {
          console.log(`   üîç DEBUG - Chamada de Fallback (Haiku):`);
          console.log(`   üéØ Modelo: ${MODELS['haiku'].id}`);
          console.log(`   ‚öôÔ∏è  maxTokens: ${MODELS['haiku'].maxTokens} (8k output limit)`);
          console.log(`   üìè Tamanho do prompt: ${Math.round(extractionPrompt.length / 1000)}k caracteres (mesmo prompt)`);

          // Fallback para Haiku (mais caro mas funciona)
          response = await conversar(extractionPrompt, {
            modelo: MODELS['haiku'].id,
            systemPrompt: 'Voc√™ √© um extrator de texto especializado. Preserve TODA informa√ß√£o, n√£o resuma.',
            temperature: 0.1,
            maxTokens: MODELS['haiku'].maxTokens,  // 8,000 tokens
            enableTools: false,
            enableCache: false
          });

          console.log(`\n   üì¶ DEBUG - Resposta Recebida do Haiku:`);
          console.log(`   ‚úÖ sucesso: ${response?.sucesso}`);
          console.log(`   üìä response.erro: ${response?.erro}`);
          console.log(`   üìä response.statusCode: ${response?.statusCode}`);
          console.log(`   üìä response keys: ${response ? Object.keys(response).join(', ') : 'null'}`);
          if (response?.resposta) {
            console.log(`   üìä response.resposta length: ${response.resposta.length} chars`);
            console.log(`   üìù Primeiros 200 chars da resposta:`, response.resposta.substring(0, 200));
          }

          if (response && response.sucesso === false) {
            console.log(`   ‚ùå FALHA CONFIRMADA - Haiku tamb√©m retornou sucesso:false`);
            throw new Error(`Both Nova Micro and Haiku failed. Last error: ${response.erro}`);
          }

          modelUsed = 'haiku';
          console.log(`   ‚úÖ Fallback para Haiku bem-sucedido`);

        } catch (haikuError) {
          console.error(`   ‚ùå‚ùå FALHA TOTAL: Ambos os modelos falharam`);
          console.error(`   Nova Micro: ${novaMicroError.message}`);
          console.error(`   Haiku: ${haikuError.message}`);
          throw haikuError;
        }
      }

      // Validar resposta
      if (!response) {
        throw new Error('Resposta do Bedrock √© null ou undefined');
      }

      // Verificar se houve erro no Bedrock
      if (response.sucesso === false) {
        console.error(`   ‚ùå Erro do Bedrock:`, response);
        throw new Error(`Bedrock error: ${response.erro || 'Unknown error'}. StatusCode: ${response.statusCode || 'N/A'}`);
      }

      if (!response.resposta) {
        console.error(`   ‚ùå Resposta do Bedrock sem campo 'resposta':`, JSON.stringify(response, null, 2));
        throw new Error(`Campo 'resposta' n√£o encontrado. Response keys: ${Object.keys(response).join(', ')}`);
      }

      const extractedText = response.resposta;
      const elapsedTime = Math.round((Date.now() - startTime) / 1000);

      const inputTokens = this.estimateTokens(rawText);
      const outputTokens = this.estimateTokens(extractedText);
      const cost = (inputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.input +
                   (outputTokens / 1_000_000) * MODELS[modelUsed].costPer1M.output;

      console.log(`   ‚úÖ Extra√ß√£o conclu√≠da em ${elapsedTime}s`);
      console.log(`   ü§ñ Modelo usado: ${MODELS[modelUsed].name}`);
      console.log(`   üìä Texto extra√≠do: ${Math.round(extractedText.length / 1000)}k caracteres`);
      console.log(`   üí∞ Custo: $${cost.toFixed(4)}`);

      // Update progress for single-pass extraction
      if (jobId) {
        await extractionProgressService.updateChunkProgress(jobId, 0, {
          inputSize: rawText.length,
          outputSize: extractedText.length,
          cost,
          time: elapsedTime,
          model: modelUsed
        });
      }

      const result = {
        extractedText,
        metadata: {
          documentId,
          documentName,
          originalSize: rawText.length,
          extractedSize: extractedText.length,
          extractedAt: new Date().toISOString(),
          model: modelUsed,
          modelName: MODELS[modelUsed].name,
          usedFallback: modelUsed !== 'nova-micro',
          inputTokens,
          outputTokens,
          cost,
          processingTime: elapsedTime
        }
      };

      // Salva em cache
      fs.writeFileSync(cachePath, JSON.stringify(result, null, 2));
      console.log(`   üíæ Extra√ß√£o salva em cache: ${cacheId}.json`);

      return result;

    } catch (error) {
      console.error(`   ‚ùå Erro na extra√ß√£o:`, error);

      // Fail job on error
      if (jobId) {
        await extractionProgressService.failJob(jobId, error.message);
      }

      throw error;
    }
  }

  /**
   * ETAPA 2: Salvamento no KB como documento intermedi√°rio
   *
   * @param {string} extractedText - Texto completo extra√≠do
   * @param {string} documentId - ID do documento original
   * @param {string} documentName - Nome do documento original
   */
  async saveExtractedTextToKB(extractedText, documentId, documentName) {
    console.log(`\nüíæ [V2 - ETAPA 2] SALVAMENTO NO KB`);

    const kbPath = path.join(ACTIVE_PATHS.data, 'kb-documents.json');
    let allDocs = [];

    if (fs.existsSync(kbPath)) {
      allDocs = JSON.parse(fs.readFileSync(kbPath, 'utf-8'));
    }

    // Cria documento intermedi√°rio
    const intermediateDoc = {
      id: `kb-extracted-${documentId}-${Date.now()}`,
      name: `${documentName} - TEXTO_COMPLETO.md`,
      originalName: documentName,
      type: 'text/markdown',
      size: extractedText.length,
      uploadedAt: new Date().toISOString(),
      textLength: extractedText.length,
      metadata: {
        isExtractedText: true,
        parentDocument: documentId,
        extractionSource: 'nova-micro',
        purpose: 'intermediate-full-text'
      }
    };

    // Salva arquivo
    const textPath = path.join(this.extractedTextCachePath, `${intermediateDoc.id}.md`);
    fs.writeFileSync(textPath, extractedText, 'utf-8');
    intermediateDoc.path = textPath;

    // Adiciona ao KB
    allDocs.push(intermediateDoc);
    fs.writeFileSync(kbPath, JSON.stringify(allDocs, null, 2));

    console.log(`   ‚úÖ Documento intermedi√°rio salvo: ${intermediateDoc.name}`);
    console.log(`   üìä Tamanho: ${Math.round(extractedText.length / 1000)}k caracteres`);
    console.log(`   üÜî ID: ${intermediateDoc.id}`);

    return intermediateDoc;
  }

  /**
   * Salva ficheiros t√©cnicos no KB e atualiza metadata do documento principal
   *
   * @param {Object} technicalFiles - Objeto com ficheiros {FICHAMENTO, ANALISE_JURIDICA, ...}
   * @param {string} documentId - ID do documento principal
   * @param {string} documentName - Nome do documento principal
   * @param {string} intermediateDocId - ID do documento texto completo
   */
  async saveTechnicalFilesToKB(technicalFiles, documentId, documentName, intermediateDocId, userId = null) {
    console.log(`\nüíæ [V2 - SALVAMENTO FICHEIROS T√âCNICOS NO KB]`);
    console.log(`   üîê userId: ${userId || 'n√£o fornecido'}`);

    const kbPath = path.join(ACTIVE_PATHS.data, 'kb-documents.json');
    const kbDocsDir = path.join(ACTIVE_PATHS.data, 'knowledge-base', 'documents');

    // Garante que diret√≥rio existe
    if (!fs.existsSync(kbDocsDir)) {
      fs.mkdirSync(kbDocsDir, { recursive: true });
    }

    let allDocs = [];
    if (fs.existsSync(kbPath)) {
      allDocs = JSON.parse(fs.readFileSync(kbPath, 'utf-8'));
    }

    const timestamp = Date.now();
    const savedFiles = [];

    // Mapear nomes de ficheiros para ordem/tipo (TODOS OS 18 TIPOS)
    const fileMapping = {
      'FICHAMENTO': { order: 1, prefix: '01_FICHAMENTO', extension: '.md', type: 'FICHAMENTO' },
      'CRONOLOGIA': { order: 2, prefix: '02_CRONOLOGIA', extension: '.md', type: 'CRONOLOGIA' },
      'LINHA_DO_TEMPO': { order: 3, prefix: '03_LINHA_DO_TEMPO', extension: '.md', type: 'LINHA_DO_TEMPO' },
      'MAPA_DE_PARTES': { order: 4, prefix: '04_MAPA_DE_PARTES', extension: '.md', type: 'MAPA_DE_PARTES' },
      'RESUMO_EXECUTIVO': { order: 5, prefix: '05_RESUMO_EXECUTIVO', extension: '.md', type: 'RESUMO_EXECUTIVO' },
      'TESES_JURIDICAS': { order: 6, prefix: '06_TESES_JURIDICAS', extension: '.md', type: 'TESES_JURIDICAS' },
      'ANALISE_DE_PROVAS': { order: 7, prefix: '07_ANALISE_DE_PROVAS', extension: '.md', type: 'ANALISE_DE_PROVAS' },
      'QUESTOES_JURIDICAS': { order: 8, prefix: '08_QUESTOES_JURIDICAS', extension: '.md', type: 'QUESTOES_JURIDICAS' },
      'PEDIDOS_E_DECISOES': { order: 9, prefix: '09_PEDIDOS_E_DECISOES', extension: '.md', type: 'PEDIDOS_E_DECISOES' },
      'RECURSOS_INTERPOSTOS': { order: 10, prefix: '10_RECURSOS_INTERPOSTOS', extension: '.md', type: 'RECURSOS_INTERPOSTOS' },
      'PRAZOS_E_INTIMACOES': { order: 11, prefix: '11_PRAZOS_E_INTIMACOES', extension: '.md', type: 'PRAZOS_E_INTIMACOES' },
      'CUSTAS_E_VALORES': { order: 12, prefix: '12_CUSTAS_E_VALORES', extension: '.md', type: 'CUSTAS_E_VALORES' },
      'JURISPRUDENCIA_CITADA': { order: 13, prefix: '13_JURISPRUDENCIA_CITADA', extension: '.md', type: 'JURISPRUDENCIA_CITADA' },
      'HISTORICO_PROCESSUAL': { order: 14, prefix: '14_HISTORICO_PROCESSUAL', extension: '.md', type: 'HISTORICO_PROCESSUAL' },
      'MANIFESTACOES_POR_PARTE': { order: 15, prefix: '15_MANIFESTACOES_POR_PARTE', extension: '.md', type: 'MANIFESTACOES_POR_PARTE' },
      'ANALISE_DE_RISCO': { order: 16, prefix: '16_ANALISE_DE_RISCO', extension: '.md', type: 'ANALISE_DE_RISCO' },
      'ESTRATEGIA_E_PROXIMOS_PASSOS': { order: 17, prefix: '17_ESTRATEGIA_E_PROXIMOS_PASSOS', extension: '.md', type: 'ESTRATEGIA_E_PROXIMOS_PASSOS' },
      'PRECEDENTES_SIMILARES': { order: 18, prefix: '18_PRECEDENTES_SIMILARES', extension: '.md', type: 'PRECEDENTES_SIMILARES' }
    };

    // Salvar cada ficheiro
    for (const [fileKey, fileContent] of Object.entries(technicalFiles)) {
      const fileInfo = fileMapping[fileKey];
      if (!fileInfo || !fileContent) continue;

      // ID √∫nico para o ficheiro
      const fileId = `${timestamp}_${documentName.replace(/\.[^/.]+$/, '')}_${fileInfo.prefix}`;
      const fileName = `${fileId}${fileInfo.extension}`;
      const filePath = path.join(kbDocsDir, fileName);

      // Salva conte√∫do
      fs.writeFileSync(filePath, fileContent, 'utf-8');

      // Cria metadata do ficheiro
      const fileDoc = {
        id: fileId,
        name: `${fileInfo.prefix}${fileInfo.extension}`,
        originalName: documentName,
        type: 'text/markdown',
        size: fileContent.length,
        uploadedAt: new Date().toISOString(),
        path: filePath,
        userId: userId,  // ‚úÖ FIX: Add userId to document metadata
        metadata: {
          isStructuredDocument: true,
          parentDocument: documentId,
          intermediateDocument: intermediateDocId,
          fileType: fileInfo.type,
          order: fileInfo.order,
          generatedBy: 'document-processor-v2',
          analysisModel: 'claude-sonnet'
        }
      };

      // Salva metadata separado
      const metadataPath = path.join(kbDocsDir, `${fileId}.metadata.json`);
      fs.writeFileSync(metadataPath, JSON.stringify(fileDoc, null, 2));

      // Adiciona ao KB
      allDocs.push(fileDoc);
      savedFiles.push({
        name: fileDoc.name,
        path: filePath,
        type: fileInfo.type,
        size: fileContent.length
      });

      console.log(`   ‚úÖ ${fileInfo.prefix}${fileInfo.extension} salvo (${Math.round(fileContent.length / 1000)}k chars)`);
    }

    // Atualiza documento principal com refer√™ncias aos ficheiros estruturados
    const mainDocIndex = allDocs.findIndex(d =>
      d.id === documentId ||
      d.metadata?.parentDocument === documentId ||
      d.originalName === documentName
    );

    if (mainDocIndex !== -1) {
      if (!allDocs[mainDocIndex].metadata) {
        allDocs[mainDocIndex].metadata = {};
      }
      allDocs[mainDocIndex].metadata.structuredDocsInKB = savedFiles;
      allDocs[mainDocIndex].metadata.hasStructuredFiles = true;
      allDocs[mainDocIndex].metadata.structuredFilesCount = savedFiles.length;
      console.log(`   ‚úÖ Metadata do documento principal atualizado`);
    }

    // Salva kb-documents.json atualizado
    fs.writeFileSync(kbPath, JSON.stringify(allDocs, null, 2));

    console.log(`\n   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
    console.log(`   ‚úÖ ${savedFiles.length} ficheiros salvos no KB`);
    console.log(`   üìÇ Diret√≥rio: knowledge-base/documents/`);
    console.log(`   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);

    return {
      success: true,
      savedFiles,
      count: savedFiles.length
    };
  }

  /**
   * ETAPA 3: An√°lise profunda com LLM Premium
   *
   * @param {string} extractedText - Texto completo j√° limpo
   * @param {string} analysisPrompt - Prompt de an√°lise do usu√°rio
   * @param {string} model - Modelo premium a usar (haiku, sonnet, opus)
   * @param {string} systemPrompt - System prompt customizado
   */
  async analyzeWithPremiumLLM(extractedText, analysisPrompt, model = 'sonnet', systemPrompt = '') {
    console.log(`\nüß† [V2 - ETAPA 3] AN√ÅLISE COM LLM PREMIUM`);
    console.log(`   Modelo: ${MODELS[model].name}`);
    console.log(`   Texto: ${Math.round(extractedText.length / 1000)}k caracteres (~${this.estimateTokens(extractedText).toLocaleString()} tokens)`);

    const startTime = Date.now();

    const fullPrompt = `
${analysisPrompt}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
DOCUMENTO COMPLETO (J√Å EXTRA√çDO E ESTRUTURADO):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

${extractedText}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
FIM DO DOCUMENTO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

FORNE√áA UMA AN√ÅLISE COMPLETA E PROFUNDA DO DOCUMENTO ACIMA:
`;

    try {
      const response = await conversar(fullPrompt, {
        modelo: MODELS[model].id,
        systemPrompt: systemPrompt || 'Voc√™ √© um assistente jur√≠dico especializado em an√°lise profunda de documentos processuais brasileiros.',
        temperature: 0.3,
        maxTokens: MODELS[model].maxTokens,  // Use model-specific output limit
        enableTools: false,
        enableCache: false
      });

      // Validar resposta
      if (!response) {
        throw new Error('Resposta do Bedrock √© null ou undefined');
      }

      // Verificar se houve erro no Bedrock
      if (response.sucesso === false) {
        console.error(`   ‚ùå Erro do Bedrock:`, response);
        throw new Error(`Bedrock error: ${response.erro || 'Unknown error'}. StatusCode: ${response.statusCode || 'N/A'}`);
      }

      if (!response.resposta) {
        console.error(`   ‚ùå Resposta do Bedrock sem campo 'resposta':`, JSON.stringify(response, null, 2));
        throw new Error(`Campo 'resposta' n√£o encontrado. Response keys: ${Object.keys(response).join(', ')}`);
      }

      const elapsedTime = Math.round((Date.now() - startTime) / 1000);

      const inputTokens = this.estimateTokens(extractedText + analysisPrompt);
      const outputTokens = this.estimateTokens(response.resposta);
      const cost = (inputTokens / 1_000_000) * MODELS[model].costPer1M.input +
                   (outputTokens / 1_000_000) * MODELS[model].costPer1M.output;

      console.log(`   ‚úÖ An√°lise conclu√≠da em ${elapsedTime}s`);
      console.log(`   üí∞ Custo: $${cost.toFixed(4)}`);

      return {
        success: true,
        analysis: response.resposta,
        metadata: {
          model,
          inputTokens,
          outputTokens,
          cost,
          processingTime: elapsedTime
        }
      };

    } catch (error) {
      console.error(`   ‚ùå Erro na an√°lise:`, error);
      return {
        success: false,
        error: error.message
      };
    }
  }

  /**
   * ETAPA 4 (BATCH): Gera√ß√£o de 20 ficheiros t√©cnicos em lote
   *
   * NOVA IMPLEMENTA√á√ÉO OTIMIZADA:
   * - 1 √∫nica chamada √† IA com prompt master
   * - IA retorna JSON com 20 an√°lises
   * - Sistema quebra em 20 arquivos .md
   * - Custo: ~$0.05 (vs $0.71 com 20 chamadas separadas)
   * - Economia: 93%
   *
   * @param {string} extractedText - Texto completo j√° limpo
   * @param {string} documentId - ID do documento
   * @param {string} documentName - Nome do documento
   * @param {string} model - Modelo premium a usar (sonnet recomendado)
   * @param {Function} progressCallback - Callback de progresso
   */
  async generateTechnicalFilesBatch(extractedText, documentId, documentName, model = 'sonnet', progressCallback = null) {
    console.log(`\nüìÑ [V2 - ETAPA 4 BATCH] GERA√á√ÉO DE 20 FICHEIROS EM LOTE`);

    // ‚ö†Ô∏è CORRE√á√ÉO: Sonnet tem limite de 8K tokens output, insuficiente para 18 fichamentos
    // Usar Opus (16K tokens) ou dividir em 2 batches
    let effectiveModel = model;
    let useSplitBatch = false;

    if (model === 'sonnet') {
      console.log(`   ‚ö†Ô∏è  AVISO: Claude Sonnet (8K tokens) pode truncar resposta com 18 fichamentos`);
      if (MODELS['opus']) {
        effectiveModel = 'opus';
        console.log(`   ‚úÖ Alternando para Claude Opus 4 (16K tokens) automaticamente`);
      } else {
        console.log(`   ‚úÖ Dividindo em 2 batches menores (9 fichamentos cada)`);
        useSplitBatch = true;
        effectiveModel = 'sonnet';
      }
    }

    console.log(`   Modelo: ${MODELS[effectiveModel].name}`);
    console.log(`   M√©todo: ${useSplitBatch ? '2 chamadas (split batch)' : '1 √∫nica chamada √† IA (otimizado)'}`);

    const startTime = Date.now();

    try {
      // Se usar split batch, chamar m√©todo alternativo
      if (useSplitBatch) {
        return await this.generateTechnicalFilesSplitBatch(extractedText, documentId, documentName, effectiveModel, progressCallback);
      }

      // Construir prompt completo
      const fullPrompt = BATCH_ANALYSIS_PROMPT + '\n\n' + extractedText;

      console.log(`   üìä Tamanho do input: ${Math.round(fullPrompt.length / 1000)}k chars (~${this.estimateTokens(fullPrompt).toLocaleString()} tokens)`);
      console.log(`   üìä Output esperado: ~10-12k tokens (18 fichamentos √ó ~600 tokens cada)`);
      console.log(`   üìä Limite do modelo: ${MODELS[effectiveModel].maxTokens} tokens`);

      if (progressCallback) {
        await progressCallback('batch_analysis', 30, 'Analisando processo completo (20 tipos)...');
      }

      // 1 √öNICA CHAMADA √Ä IA
      console.log(`   ü§ñ Chamando IA para an√°lise completa...`);
      const response = await this.analyzeWithPremiumLLM(
        fullPrompt,
        '', // Prompt j√° est√° no fullPrompt
        effectiveModel,
        'Voc√™ √© um assistente jur√≠dico especializado. Retorne APENAS o JSON solicitado, sem texto adicional antes ou depois. IMPORTANTE: Complete TODOS os 18 tipos de fichamento, n√£o truncar.'
      );

      if (!response.success) {
        throw new Error(`Erro na an√°lise: ${response.error || 'Resposta n√£o dispon√≠vel'}`);
      }

      const totalTokens = response.metadata.inputTokens + response.metadata.outputTokens;
      console.log(`   ‚úÖ IA respondeu (${totalTokens.toLocaleString()} tokens, $${response.metadata.cost.toFixed(4)})`);

      // VERIFICAR TRUNCAMENTO
      const responseLength = response.analysis.length;
      const estimatedOutputTokens = this.estimateTokens(response.analysis);
      console.log(`   üìä Output recebido: ${Math.round(responseLength / 1000)}k chars (~${estimatedOutputTokens.toLocaleString()} tokens)`);

      if (estimatedOutputTokens >= MODELS[effectiveModel].maxTokens * 0.95) {
        console.log(`   ‚ö†Ô∏è  ALERTA: Resposta pr√≥xima ao limite de tokens, pode estar truncada!`);
      }

      if (progressCallback) {
        await progressCallback('parsing_json', 60, 'Processando resposta da IA...');
      }

      // PARSEAR JSON
      console.log(`   üì¶ Parseando JSON da resposta...`);
      let analysisData;

      try {
        // Remover markdown code blocks se houver
        let cleanedResponse = response.analysis.trim();

        // Remover ```json e ``` se presente
        if (cleanedResponse.startsWith('```json')) {
          cleanedResponse = cleanedResponse.replace(/^```json\s*/, '').replace(/```\s*$/, '');
        } else if (cleanedResponse.startsWith('```')) {
          cleanedResponse = cleanedResponse.replace(/^```\s*/, '').replace(/```\s*$/, '');
        }

        analysisData = JSON.parse(cleanedResponse);
        console.log(`   ‚úÖ JSON parseado com sucesso`);
        console.log(`   üìä Chaves encontradas: ${Object.keys(analysisData).length}`);
      } catch (parseError) {
        console.error(`   ‚ùå Erro ao parsear JSON:`, parseError.message);
        console.log(`   üìÑ Primeiros 1000 chars da resposta:`, response.analysis.substring(0, 1000));
        console.log(`   üìÑ √öltimos 1000 chars da resposta:`, response.analysis.substring(Math.max(0, response.analysis.length - 1000)));
        console.log(`   üìä Tamanho total da resposta: ${response.analysis.length} chars`);

        // DETECTAR TRUNCAMENTO: √∫ltimos chars n√£o terminam com }
        const lastChars = response.analysis.trim().slice(-50);
        if (!lastChars.endsWith('}') && !lastChars.endsWith('}```')) {
          console.log(`   ‚ö†Ô∏è  DIAGN√ìSTICO: Resposta truncada! √öltimos chars: "${lastChars}"`);
          console.log(`   üí° SOLU√á√ÉO: Reprocessando com split batch...`);
          return await this.generateTechnicalFilesSplitBatch(extractedText, documentId, documentName, effectiveModel, progressCallback);
        }

        // Fallback: tentar extrair JSON da resposta
        const jsonMatch = response.analysis.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          try {
            analysisData = JSON.parse(jsonMatch[0]);
            console.log(`   ‚ö†Ô∏è  JSON extra√≠do com regex - parsear bem-sucedido`);
          } catch {
            throw new Error('Resposta da IA n√£o est√° em formato JSON v√°lido');
          }
        } else {
          throw new Error('N√£o foi poss√≠vel extrair JSON da resposta');
        }
      }

      if (progressCallback) {
        await progressCallback('creating_files', 80, 'Criando 20 arquivos estruturados...');
      }

      // QUEBRAR EM 20 ARQUIVOS
      console.log(`   üìù Criando 20 arquivos .md individuais...`);
      const files = {};

      const fileTypes = [
        'FICHAMENTO',
        'CRONOLOGIA',
        'LINHA_DO_TEMPO',
        'MAPA_DE_PARTES',
        'RESUMO_EXECUTIVO',
        'TESES_JURIDICAS',
        'ANALISE_DE_PROVAS',
        'QUESTOES_JURIDICAS',
        'PEDIDOS_E_DECISOES',
        'RECURSOS_INTERPOSTOS',
        'PRAZOS_E_INTIMACOES',
        'CUSTAS_E_VALORES',
        'JURISPRUDENCIA_CITADA',
        'HISTORICO_PROCESSUAL',
        'MANIFESTACOES_POR_PARTE',
        'ANALISE_DE_RISCO',
        'ESTRATEGIA_E_PROXIMOS_PASSOS',
        'PRECEDENTES_SIMILARES'
      ];

      let filesCreated = 0;
      let emptyFiles = 0;
      for (const fileType of fileTypes) {
        if (analysisData[fileType] && analysisData[fileType].trim().length > 50) {
          files[fileType] = analysisData[fileType];
          filesCreated++;
          console.log(`      ‚úÖ ${fileType}.md (${Math.round(analysisData[fileType].length / 1000)}KB)`);
        } else {
          console.log(`      ‚ö†Ô∏è  ${fileType}.md - [N√ÉO GERADO OU VAZIO]`);
          emptyFiles++;
          // Criar arquivo placeholder
          files[fileType] = `# ${fileType.replace(/_/g, ' ')}\n\n[INFORMA√á√ïES INSUFICIENTES NO DOCUMENTO PARA GERAR ESTA AN√ÅLISE]\n\nA IA n√£o conseguiu extrair informa√ß√µes suficientes do processo para gerar este tipo de an√°lise.`;
        }
      }

      const totalTime = Math.round((Date.now() - startTime) / 1000);
      const totalCost = response.metadata.cost;

      console.log(`\n   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
      console.log(`   ‚úÖ ${filesCreated}/18 ficheiros gerados com conte√∫do`);
      if (emptyFiles > 0) {
        console.log(`   ‚ö†Ô∏è  ${emptyFiles} ficheiros vazios/incompletos`);
      }
      console.log(`   ‚è±Ô∏è  Tempo total: ${totalTime}s`);
      console.log(`   üí∞ Custo total: $${totalCost.toFixed(4)}`);
      console.log(`   üí° Economia vs m√©todo antigo: ~93%`);
      console.log(`   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);

      // Se muitos arquivos vazios, retornar erro para acionar fallback
      if (emptyFiles > 9) {
        console.log(`   ‚ö†Ô∏è  MUITOS ARQUIVOS VAZIOS - Acionando fallback split batch...`);
        return await this.generateTechnicalFilesSplitBatch(extractedText, documentId, documentName, effectiveModel, progressCallback);
      }

      return {
        success: true,
        files,
        metadata: {
          filesGenerated: filesCreated,
          emptyFiles,
          totalCost,
          totalTime,
          method: 'batch',
          model: MODELS[effectiveModel].name
        }
      };

    } catch (error) {
      console.error(`   ‚ùå Erro na gera√ß√£o em lote:`, error);
      return {
        success: false,
        error: error.message,
        files: {},
        metadata: {
          filesGenerated: 0,
          totalCost: 0,
          totalTime: Math.round((Date.now() - startTime) / 1000)
        }
      };
    }
  }

  /**
   * ETAPA 4 SPLIT: Gera√ß√£o em 2 lotes (9 fichamentos cada)
   *
   * Usado quando modelo tem limite de output tokens insuficiente (ex: Sonnet 8K)
   * Divide os 18 fichamentos em 2 batches de 9 cada
   */
  async generateTechnicalFilesSplitBatch(extractedText, documentId, documentName, model = 'sonnet', progressCallback = null) {
    console.log(`\nüìÑ [V2 - ETAPA 4 SPLIT BATCH] GERA√á√ÉO EM 2 LOTES`);
    console.log(`   Modelo: ${MODELS[model].name}`);
    console.log(`   M√©todo: 2 chamadas (9 fichamentos cada)`);

    const startTime = Date.now();
    let totalCost = 0;

    try {
      const allFiles = {};
      const fileTypes = [
        'FICHAMENTO',
        'CRONOLOGIA',
        'LINHA_DO_TEMPO',
        'MAPA_DE_PARTES',
        'RESUMO_EXECUTIVO',
        'TESES_JURIDICAS',
        'ANALISE_DE_PROVAS',
        'QUESTOES_JURIDICAS',
        'PEDIDOS_E_DECISOES',
        'RECURSOS_INTERPOSTOS',
        'PRAZOS_E_INTIMACOES',
        'CUSTAS_E_VALORES',
        'JURISPRUDENCIA_CITADA',
        'HISTORICO_PROCESSUAL',
        'MANIFESTACOES_POR_PARTE',
        'ANALISE_DE_RISCO',
        'ESTRATEGIA_E_PROXIMOS_PASSOS',
        'PRECEDENTES_SIMILARES'
      ];

      // Dividir em 2 batches
      const batch1Types = fileTypes.slice(0, 9);
      const batch2Types = fileTypes.slice(9, 18);

      // BATCH 1
      console.log(`\n   üì¶ LOTE 1/2: ${batch1Types.length} fichamentos`);
      if (progressCallback) {
        await progressCallback('batch_1', 30, 'Gerando lote 1/2 (9 fichamentos)...');
      }

      const prompt1 = this.createSplitBatchPrompt(batch1Types);
      const fullPrompt1 = prompt1 + '\n\n' + extractedText;

      const response1 = await this.analyzeWithPremiumLLM(
        fullPrompt1,
        '',
        model,
        'Voc√™ √© um assistente jur√≠dico especializado. Retorne APENAS o JSON solicitado com os 9 tipos de fichamento, sem texto adicional.'
      );

      if (!response1.success) {
        throw new Error(`Erro no lote 1: ${response1.error}`);
      }

      totalCost += response1.metadata.cost;
      console.log(`   ‚úÖ Lote 1 conclu√≠do ($${response1.metadata.cost.toFixed(4)})`);

      // Parse batch 1
      let batch1Data;
      try {
        let cleaned1 = response1.analysis.trim();
        if (cleaned1.startsWith('```json')) {
          cleaned1 = cleaned1.replace(/^```json\s*/, '').replace(/```\s*$/, '');
        } else if (cleaned1.startsWith('```')) {
          cleaned1 = cleaned1.replace(/^```\s*/, '').replace(/```\s*$/, '');
        }
        batch1Data = JSON.parse(cleaned1);
        console.log(`   ‚úÖ Lote 1 parseado: ${Object.keys(batch1Data).length} fichamentos`);
      } catch (parseError) {
        console.error(`   ‚ùå Erro ao parsear lote 1:`, parseError.message);
        batch1Data = {};
      }

      // BATCH 2
      console.log(`\n   üì¶ LOTE 2/2: ${batch2Types.length} fichamentos`);
      if (progressCallback) {
        await progressCallback('batch_2', 60, 'Gerando lote 2/2 (9 fichamentos)...');
      }

      const prompt2 = this.createSplitBatchPrompt(batch2Types);
      const fullPrompt2 = prompt2 + '\n\n' + extractedText;

      const response2 = await this.analyzeWithPremiumLLM(
        fullPrompt2,
        '',
        model,
        'Voc√™ √© um assistente jur√≠dico especializado. Retorne APENAS o JSON solicitado com os 9 tipos de fichamento, sem texto adicional.'
      );

      if (!response2.success) {
        throw new Error(`Erro no lote 2: ${response2.error}`);
      }

      totalCost += response2.metadata.cost;
      console.log(`   ‚úÖ Lote 2 conclu√≠do ($${response2.metadata.cost.toFixed(4)})`);

      // Parse batch 2
      let batch2Data;
      try {
        let cleaned2 = response2.analysis.trim();
        if (cleaned2.startsWith('```json')) {
          cleaned2 = cleaned2.replace(/^```json\s*/, '').replace(/```\s*$/, '');
        } else if (cleaned2.startsWith('```')) {
          cleaned2 = cleaned2.replace(/^```\s*/, '').replace(/```\s*$/, '');
        }
        batch2Data = JSON.parse(cleaned2);
        console.log(`   ‚úÖ Lote 2 parseado: ${Object.keys(batch2Data).length} fichamentos`);
      } catch (parseError) {
        console.error(`   ‚ùå Erro ao parsear lote 2:`, parseError.message);
        batch2Data = {};
      }

      if (progressCallback) {
        await progressCallback('merging', 80, 'Mesclando resultados...');
      }

      // Mesclar resultados
      let filesCreated = 0;
      for (const fileType of fileTypes) {
        if (batch1Data[fileType]) {
          allFiles[fileType] = batch1Data[fileType];
          filesCreated++;
          console.log(`      ‚úÖ ${fileType}.md (lote 1)`);
        } else if (batch2Data[fileType]) {
          allFiles[fileType] = batch2Data[fileType];
          filesCreated++;
          console.log(`      ‚úÖ ${fileType}.md (lote 2)`);
        } else {
          console.log(`      ‚ö†Ô∏è  ${fileType}.md - [N√ÉO GERADO]`);
          allFiles[fileType] = `# ${fileType.replace(/_/g, ' ')}\n\n[INFORMA√á√ïES INSUFICIENTES NO DOCUMENTO]\n\nA IA n√£o conseguiu extrair informa√ß√µes suficientes para gerar este tipo de an√°lise.`;
        }
      }

      const totalTime = Math.round((Date.now() - startTime) / 1000);

      console.log(`\n   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
      console.log(`   ‚úÖ ${filesCreated}/18 ficheiros gerados com sucesso`);
      console.log(`   ‚è±Ô∏è  Tempo total: ${totalTime}s`);
      console.log(`   üí∞ Custo total: $${totalCost.toFixed(4)}`);
      console.log(`   üìä M√©todo: Split batch (2 lotes)`);
      console.log(`   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);

      return {
        success: true,
        files: allFiles,
        metadata: {
          filesGenerated: filesCreated,
          totalCost,
          totalTime,
          method: 'split-batch',
          model: MODELS[model].name,
          batches: 2
        }
      };

    } catch (error) {
      console.error(`   ‚ùå Erro na gera√ß√£o split batch:`, error);
      return {
        success: false,
        error: error.message,
        files: {},
        metadata: {
          filesGenerated: 0,
          totalCost,
          totalTime: Math.round((Date.now() - startTime) / 1000)
        }
      };
    }
  }

  /**
   * Cria prompt para split batch com tipos espec√≠ficos
   */
  createSplitBatchPrompt(fileTypes) {
    const typesJson = fileTypes.map(type => `  "${type}": "# ${type.replace(/_/g, ' ')}\\n\\n..."`).join(',\n');

    return `Voc√™ √© um assistente jur√≠dico especializado em an√°lise processual completa.

TAREFA: Analisar o processo jur√≠dico fornecido e gerar ${fileTypes.length} TIPOS de documentos estruturados.

IMPORTANTE:
- Extraia informa√ß√µes APENAS do texto fornecido
- Seja preciso com datas, valores e nomes
- Use "[N√ÉO IDENTIFICADO]" se informa√ß√£o n√£o estiver dispon√≠vel
- Mantenha estrutura markdown de cada documento
- Seja objetivo mas completo

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
ESTRUTURA DE RESPOSTA (JSON)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Retorne um JSON com esta estrutura exata:

{
${typesJson}
}

INSTRU√á√ïES FINAIS:
1. Retorne APENAS o JSON, sem texto adicional
2. Escape quebras de linha como \\n
3. Use aspas duplas corretamente
4. Mantenha formata√ß√£o markdown dentro de cada string
5. Se uma se√ß√£o n√£o tiver informa√ß√µes suficientes, inclua "## [Se√ß√£o]\\n\\n[INFORMA√á√ïES INSUFICIENTES NO DOCUMENTO]"

IN√çCIO DO JSON:`;
  }

  /**
   * ETAPA 4 (INDIVIDUAL): Gera√ß√£o de m√∫ltiplos ficheiros t√©cnicos (M√âTODO ANTIGO)
   *
   * NOTA: Este m√©todo faz chamadas separadas para cada tipo de an√°lise.
   * Use generateTechnicalFilesBatch() para melhor custo-benef√≠cio.
   *
   * @param {string} extractedText - Texto completo j√° limpo
   * @param {string} documentId - ID do documento
   * @param {string} documentName - Nome do documento
   * @param {string} model - Modelo premium a usar
   */
  async generateTechnicalFiles(extractedText, documentId, documentName, model = 'sonnet', progressCallback = null) {
    console.log(`\nüìÑ [V2 - ETAPA 4] GERA√á√ÉO DE FICHEIROS T√âCNICOS`);
    console.log(`   Modelo: ${MODELS[model].name}`);

    const files = {};
    const costs = [];
    const startTime = Date.now();

    // Ficheiro 1: FICHAMENTO ESTRUTURADO (20-40%)
    if (progressCallback) {
      await progressCallback('fichamento', 20, 'Gerando FICHAMENTO.md...');
    }
    console.log(`\n   üìã Gerando FICHAMENTO.md...`);
    const fichamentoPrompt = `
Crie um FICHAMENTO ESTRUTURADO completo do documento processual, seguindo o formato:

# FICHAMENTO - ${documentName}

## 1. IDENTIFICA√á√ÉO
- N√∫mero do Processo:
- Classe:
- √ìrg√£o Julgador:
- Distribui√ß√£o:
- Valor da Causa:
- Assunto:

## 2. PARTES
### Polo Ativo:
### Polo Passivo:

## 3. PEDIDOS
[Liste todos os pedidos com numera√ß√£o]

## 4. CAUSA DE PEDIR
[Fatos e fundamentos]

## 5. FUNDAMENTA√á√ÉO JUR√çDICA
[Dispositivos legais citados]

## 6. JURISPRUD√äNCIA INVOCADA
[Precedentes mencionados]

## 7. DOCUMENTOS ANEXOS
[Lista de documentos juntados]

## 8. MOVIMENTA√á√ÉO PROCESSUAL
[Principais eventos com datas]

## 9. DECIS√ïES IMPORTANTES
[Despachos, decis√µes interlocut√≥rias, senten√ßas]

## 10. VALOR ECON√îMICO
[Valores envolvidos, custas, honor√°rios]

Seja COMPLETO e DETALHADO.
`;

    const fichamento = await this.analyzeWithPremiumLLM(extractedText, fichamentoPrompt, model, 'Voc√™ √© um assistente especializado em fichamento de processos judiciais.');

    if (fichamento.success) {
      files.FICHAMENTO = fichamento.analysis;
      costs.push(fichamento.metadata.cost);
      console.log(`   ‚úÖ FICHAMENTO.md gerado ($${fichamento.metadata.cost.toFixed(4)})`);
    }

    // Ficheiro 2: AN√ÅLISE JUR√çDICA T√âCNICA (40-60%)
    if (progressCallback) {
      await progressCallback('analise', 40, 'Gerando ANALISE_JURIDICA.md...');
    }
    console.log(`\n   ‚öñÔ∏è Gerando ANALISE_JURIDICA.md...`);
    const analisePrompt = `
Fa√ßa uma AN√ÅLISE JUR√çDICA T√âCNICA profunda do documento, incluindo:

# AN√ÅLISE JUR√çDICA - ${documentName}

## 1. RESUMO EXECUTIVO
[S√≠ntese em 3-5 par√°grafos]

## 2. AN√ÅLISE DA CAUSA DE PEDIR
[An√°lise cr√≠tica dos fundamentos f√°ticos]

## 3. AN√ÅLISE DOS PEDIDOS
[Viabilidade jur√≠dica de cada pedido]

## 4. FUNDAMENTA√á√ÉO LEGAL
### Dispositivos Citados:
### Adequa√ß√£o da Fundamenta√ß√£o:
### Legisla√ß√£o Aplic√°vel N√£o Citada:

## 5. JURISPRUD√äNCIA
### Precedentes Citados:
### An√°lise dos Precedentes:
### Sugest√µes de Jurisprud√™ncia Adicional:

## 6. PONTOS FORTES
[Liste os pontos fortes da argumenta√ß√£o]

## 7. PONTOS FRACOS / VULNERABILIDADES
[Identifique fragilidades argumentativas]

## 8. ESTRAT√âGIA PROCESSUAL
[Avalie a estrat√©gia adotada]

## 9. RISCOS E OPORTUNIDADES
### Riscos:
### Oportunidades:

## 10. RECOMENDA√á√ïES
[Sugest√µes estrat√©gicas]

Seja CR√çTICO, T√âCNICO e FUNDAMENTADO.
`;

    const analise = await this.analyzeWithPremiumLLM(extractedText, analisePrompt, model, 'Voc√™ √© um advogado s√™nior especializado em an√°lise cr√≠tica de pe√ßas processuais.');

    if (analise.success) {
      files.ANALISE_JURIDICA = analise.analysis;
      costs.push(analise.metadata.cost);
      console.log(`   ‚úÖ ANALISE_JURIDICA.md gerado ($${analise.metadata.cost.toFixed(4)})`);
    }

    // Ficheiro 3: CRONOLOGIA DETALHADA (60-75%)
    if (progressCallback) {
      await progressCallback('cronologia', 60, 'Gerando CRONOLOGIA.md...');
    }
    console.log(`\n   üìÖ Gerando CRONOLOGIA.md...`);
    const cronologiaPrompt = `
Crie uma LINHA DO TEMPO COMPLETA do processo, extraindo TODAS as datas e eventos:

# CRONOLOGIA - ${documentName}

| Data | Evento | Respons√°vel | Observa√ß√µes |
|------|--------|-------------|-------------|
| DD/MM/AAAA | [Evento] | [Quem] | [Detalhes] |

Ap√≥s a tabela, forne√ßa:

## AN√ÅLISE TEMPORAL

### Prazos Cumpridos:
### Prazos Descumpridos:
### Eventos Cr√≠ticos:
### Per√≠odos de In√©rcia:
### Dura√ß√£o Total:

Seja EXAUSTIVO - extraia TODAS as datas mencionadas.
`;

    const cronologia = await this.analyzeWithPremiumLLM(extractedText, cronologiaPrompt, model, 'Voc√™ √© um assistente especializado em an√°lise temporal de processos.');

    if (cronologia.success) {
      files.CRONOLOGIA = cronologia.analysis;
      costs.push(cronologia.metadata.cost);
      console.log(`   ‚úÖ CRONOLOGIA.md gerado ($${cronologia.metadata.cost.toFixed(4)})`);
    }

    // Ficheiro 4: RESUMO EXECUTIVO (75-90%)
    if (progressCallback) {
      await progressCallback('resumo', 75, 'Gerando RESUMO_EXECUTIVO.md...');
    }
    console.log(`\n   üìù Gerando RESUMO_EXECUTIVO.md...`);
    const resumoPrompt = `
Crie um RESUMO EXECUTIVO sint√©tico para leitura r√°pida por tomadores de decis√£o:

# RESUMO EXECUTIVO - ${documentName}

## ‚öñÔ∏è NATUREZA
[1-2 frases sobre o tipo de a√ß√£o]

## üë• PARTES
**Autor:** [Nome]
**R√©u:** [Nome]

## üí∞ VALOR
R$ [valor] ([extenso])

## üìã PEDIDOS PRINCIPAIS
1. [Pedido 1]
2. [Pedido 2]
3. [Pedido 3]

## üéØ CAUSA DE PEDIR (Resumo)
[2-3 par√°grafos sint√©ticos]

## ‚öñÔ∏è FUNDAMENTA√á√ÉO JUR√çDICA
- [Lei X, art. Y]
- [Lei Z, art. W]

## üìä STATUS ATUAL
[Fase processual e √∫ltima movimenta√ß√£o]

## ‚ö†Ô∏è PONTOS DE ATEN√á√ÉO
- [Ponto cr√≠tico 1]
- [Ponto cr√≠tico 2]

## üìà PROGN√ìSTICO
[Avalia√ß√£o sint√©tica de chances de √™xito]

---
**Gerado em:** [Data]
**Analista:** ROM Agent (IA)

M√°ximo 2 p√°ginas. Seja SINT√âTICO e OBJETIVO.
`;

    const resumo = await this.analyzeWithPremiumLLM(extractedText, resumoPrompt, model, 'Voc√™ √© um analista que cria resumos executivos para advogados s√™niores.');

    if (resumo.success) {
      files.RESUMO_EXECUTIVO = resumo.analysis;
      costs.push(resumo.metadata.cost);
      console.log(`   ‚úÖ RESUMO_EXECUTIVO.md gerado ($${resumo.metadata.cost.toFixed(4)})`);
    }

    const totalTime = Math.round((Date.now() - startTime) / 1000);
    const totalCost = costs.reduce((sum, c) => sum + c, 0);

    console.log(`\n   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
    console.log(`   ‚úÖ ${Object.keys(files).length} ficheiros gerados`);
    console.log(`   ‚è±Ô∏è Tempo total: ${totalTime}s`);
    console.log(`   üí∞ Custo total: $${totalCost.toFixed(4)}`);
    console.log(`   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);

    return {
      success: true,
      files,
      metadata: {
        filesGenerated: Object.keys(files).length,
        totalCost,
        totalTime
      }
    };
  }

  /**
   * M√âTODO PRINCIPAL: Processa documento completo (todas as 4 etapas)
   *
   * @param {string} rawText - Texto bruto do PDF
   * @param {string} documentId - ID do documento
   * @param {string} documentName - Nome do documento
   * @param {Object} options - Op√ß√µes de processamento
   */
  async processComplete(rawText, documentId, documentName, options = {}) {
    // Helper para logar mem√≥ria
    const logMemory = (stage) => {
      const used = process.memoryUsage();
      console.log(`   üíæ [${stage}] Mem√≥ria: RSS=${Math.round(used.rss/1024/1024)}MB, Heap=${Math.round(used.heapUsed/1024/1024)}MB/${Math.round(used.heapTotal/1024/1024)}MB`);
    };

    console.log(`\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó`);
    console.log(`‚ïë  üìÑ DOCUMENT PROCESSOR V2 - ARQUITETURA MELHORADA           ‚ïë`);
    console.log(`‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù`);
    console.log(`\nüìÑ Documento: ${documentName}`);
    console.log(`üìä Tamanho: ${Math.round(rawText.length / 1000)}k caracteres (~${this.estimateTokens(rawText).toLocaleString()} tokens)`);
    logMemory('IN√çCIO');

    const {
      extractionModel = 'nova-micro',
      analysisModel = 'sonnet',
      generateFiles = true,
      saveToKB = true,
      userId = null,  // ‚úÖ FIX: Extract userId from options
      progressCallback = null
    } = options;

    const totalStartTime = Date.now();
    const costs = [];

    try {
      // ETAPA 1: Extra√ß√£o (0-15%)
      if (progressCallback) {
        await progressCallback('extraction', 0, 'Extraindo texto com Nova Micro...');
      }

      console.log(`\nüîç [ETAPA 1] Iniciando extra√ß√£o de texto...`);
      logMemory('PR√â-EXTRA√á√ÉO');

      const extraction = await this.extractFullText(rawText, documentId, documentName);
      costs.push(extraction.metadata.cost);

      logMemory('P√ìS-EXTRA√á√ÉO');
      console.log(`   ‚úÖ Extra√ß√£o completa: ${Math.round(extraction.extractedText.length/1000)}k chars`);

      // ETAPA 2: Salvamento no KB (15-20%)
      if (progressCallback) {
        await progressCallback('saving', 15, 'Salvando texto extra√≠do no KB...');
      }

      console.log(`\nüíæ [ETAPA 2] Salvando texto extra√≠do no KB...`);
      logMemory('PR√â-SALVAMENTO');

      let intermediateDoc = null;
      if (saveToKB) {
        intermediateDoc = await this.saveExtractedTextToKB(
          extraction.extractedText,
          documentId,
          documentName
        );
        console.log(`   ‚úÖ Texto salvo no KB: ${intermediateDoc.id}`);
      }

      logMemory('P√ìS-SALVAMENTO');

      // ETAPA 2.5: Persist√™ncia completa de extra√ß√£o (20-25%)
      let completeExtractionResult = null;
      if (saveToKB) {
        if (progressCallback) {
          await progressCallback('persistence', 20, 'Salvando extra√ß√£o completa (texto + imagens + √°udio)...');
        }

        console.log(`\nüíæ [ETAPA 2.5] Persist√™ncia completa de extra√ß√£o...`);
        logMemory('PR√â-PERSIST√äNCIA');

        try {
          // üåç EXTRA√á√ÉO UNIVERSAL: Suporta QUALQUER tipo de arquivo
          // - Se options.filePath fornecido ‚Üí usa extrator universal
          // - Se apenas options.pdfPath ‚Üí mant√©m compatibilidade com PDF
          if (options.filePath) {
            console.log(`   üåç Usando EXTRATOR UNIVERSAL para arquivo: ${options.filePath}`);

            completeExtractionResult = await extractionPersistenceManager.extractAnyFileUniversal(
              options.filePath,
              documentId,
              documentName,
              {
                analyzeFrames: options.analyzeFrames !== false, // Analisar frames de v√≠deo por padr√£o
                fps: options.fps || 1 // 1 frame por segundo
              }
            );
          } else if (options.pdfPath) {
            // Compatibilidade retroativa com PDF
            console.log(`   üìÑ Usando extrator de PDF para: ${options.pdfPath}`);

            completeExtractionResult = await extractionPersistenceManager.persistCompleteExtraction(
              documentId,
              documentName,
              extraction.extractedText,
              {
                pdfPath: options.pdfPath,
                audioFiles: options.audioFiles || [],
                cost: extraction.metadata.cost,
                processingTime: extraction.metadata.processingTime,
                method: extraction.metadata.model
              }
            );
          } else {
            // Apenas texto, sem arquivo f√≠sico
            console.log(`   üìù Salvando apenas texto extra√≠do`);

            completeExtractionResult = await extractionPersistenceManager.persistCompleteExtraction(
              documentId,
              documentName,
              extraction.extractedText,
              {
                cost: extraction.metadata.cost,
                processingTime: extraction.metadata.processingTime,
                method: extraction.metadata.model
              }
            );
          }

          console.log(`   ‚úÖ Persist√™ncia completa conclu√≠da`);
          console.log(`   üìÇ Estrutura: extractions/${documentId}/`);
          console.log(`   üìä Texto: ${Math.round(completeExtractionResult.extractionData.textSize / 1000)}KB`);
          console.log(`   üñºÔ∏è  Imagens: ${completeExtractionResult.extractionData.imagesCount}`);
          console.log(`   üé§ √Åudio: ${completeExtractionResult.extractionData.audioCount || 0}`);
        } catch (persistenceError) {
          console.error(`   ‚ö†Ô∏è  Erro na persist√™ncia completa (n√£o-cr√≠tico):`, persistenceError.message);
          // N√£o falhar o processo inteiro se persist√™ncia falhar
        }

        logMemory('P√ìS-PERSIST√äNCIA');
      }

      // ETAPA 3-6: Gera√ß√£o de ficheiros t√©cnicos (25-90%)
      let technicalFiles = null;
      let savedFilesResult = null;
      if (generateFiles) {
        console.log(`\nüìù [ETAPA 3-6] Gerando ficheiros t√©cnicos com IA...`);
        logMemory('PR√â-GERA√á√ÉO-FICHEIROS');

        if (progressCallback) {
          await progressCallback('fichamento', 20, 'Gerando FICHAMENTO.md...');
        }

        // Usar m√©todo BATCH otimizado (20 an√°lises em 1 chamada)
        technicalFiles = await this.generateTechnicalFilesBatch(
          extraction.extractedText,
          documentId,
          documentName,
          analysisModel,
          progressCallback
        );
        costs.push(technicalFiles.metadata.totalCost);

        // ETAPA 7: Salvar ficheiros t√©cnicos no KB (90-100%)
        if (progressCallback) {
          await progressCallback('saving_files', 90, 'Salvando ficheiros estruturados no KB...');
        }

        if (saveToKB && technicalFiles.success && technicalFiles.files) {
          savedFilesResult = await this.saveTechnicalFilesToKB(
            technicalFiles.files,
            documentId,
            documentName,
            intermediateDoc?.id || documentId,
            userId  // ‚úÖ FIX: Pass userId to saveTechnicalFilesToKB
          );
        }
      }

      const totalTime = Math.round((Date.now() - totalStartTime) / 1000);
      const totalCost = costs.reduce((sum, c) => sum + c, 0);

      console.log(`\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó`);
      console.log(`‚ïë  ‚úÖ PROCESSAMENTO COMPLETO CONCLU√çDO                         ‚ïë`);
      console.log(`‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù`);
      console.log(`\n‚è±Ô∏è  Tempo total: ${totalTime}s`);
      console.log(`üí∞ Custo total: $${totalCost.toFixed(4)}`);
      console.log(`\nüì¶ Resultados:`);
      console.log(`   1. Texto completo extra√≠do e salvo no KB`);
      if (completeExtractionResult) {
        console.log(`   2. Extra√ß√£o completa persistida (texto + imagens + √°udio)`);
        console.log(`      üìÇ extractions/${documentId}/`);
      }
      if (technicalFiles) {
        console.log(`   3. ${technicalFiles.metadata.filesGenerated} ficheiros t√©cnicos gerados`);
      }
      if (savedFilesResult && savedFilesResult.success) {
        console.log(`   4. ${savedFilesResult.count} ficheiros salvos no KB (dispon√≠veis para chat)`);
      }
      console.log(`\nüí° Vantagens:`);
      console.log(`   ‚úÖ Texto extra√≠do reutiliz√°vel (cache)`);
      console.log(`   ‚úÖ Extra√ß√£o completa persistida com imagens e √°udio`);
      console.log(`   ‚úÖ Economia vs abordagem 100% Claude: ~50%`);
      console.log(`   ‚úÖ Ficheiros profissionais prontos para uso`);
      console.log(`   ‚úÖ KB Loader carrega automaticamente no chat`);

      return {
        success: true,
        extraction: extraction.metadata,
        intermediateDoc,
        completeExtraction: completeExtractionResult || null,
        technicalFiles: technicalFiles?.files || null,
        savedFiles: savedFilesResult?.savedFiles || [],
        metadata: {
          totalTime,
          totalCost,
          extractionCost: extraction.metadata.cost,
          analysisCost: technicalFiles?.metadata.totalCost || 0,
          filesGenerated: technicalFiles?.metadata.filesGenerated || 0,
          filesSavedToKB: savedFilesResult?.count || 0,
          completeExtractionPaths: completeExtractionResult?.paths || null
        }
      };

    } catch (error) {
      console.error(`\n‚ùå Erro no processamento V2:`, error);
      return {
        success: false,
        error: error.message
      };
    }
  }
}

// Singleton
export const documentProcessorV2 = new DocumentProcessorV2();
