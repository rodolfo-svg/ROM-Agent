/**
 * ROM Agent - MÃ³dulo AWS Bedrock
 * IntegraÃ§Ã£o com modelos de IA da AWS (Claude, Nova, Llama, etc)
 *
 * @version 1.0.0
 */

import {
  BedrockRuntimeClient,
  ConverseCommand,
  ConverseStreamCommand,
  InvokeModelCommand
} from '@aws-sdk/client-bedrock-runtime';

import {
  BedrockClient,
  ListFoundationModelsCommand,
  ListInferenceProfilesCommand
} from '@aws-sdk/client-bedrock';

// Validar credenciais AWS no startup
if (!process.env.AWS_ACCESS_KEY_ID || !process.env.AWS_SECRET_ACCESS_KEY) {
  throw new Error('âŒ AWS credentials not configured! Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY');
}

// IntegraÃ§Ã£o com ROM Tools (KB, JurisprudÃªncia, CNJ)
import { BEDROCK_TOOLS, executeTool } from './bedrock-tools.js';

// Context Manager para limitaÃ§Ã£o inteligente de tokens
import contextManager from '../utils/context-manager.js';

// Loop Guardrails para prevenÃ§Ã£o de loops infinitos
import { loopGuardrails } from '../utils/loop-guardrails.js';

// Retry logic with exponential backoff
import { retryAwsCommand } from '../utils/retry-with-backoff.js';

// Bottleneck para controle de concorrÃªncia e fila
import bottleneck from '../utils/bottleneck.js';

// Resilient Invoke: Circuit Breaker + Fallback + Retry + Bottleneck
import { resilientInvoke } from '../utils/resilient-invoke.js';

// Multi-Level Cache for 10-50x performance improvement
import { getCache } from '../utils/multi-level-cache.js';

// Model Capabilities Detection para multi-model compatibility
import { shouldEnableTools, getToolsUnavailableMessage, getModelCapabilities } from '../utils/model-capabilities.js';

// v2.9.0: Stub metrics para monitoramento (metrics module pode nao existir ainda)
const metrics = {
  incrementTotalRequests: () => {},
  observeToolLoops: () => {},
  incrementForcedPresentations: () => {},
  observeSseStreamingTime: (ms) => console.log(`[Metrics] SSE time: ${ms}ms`)
};

// ============================================================
// CONFIGURAÃ‡ÃƒO
// ============================================================

/**
 * Seleciona modelo padrÃ£o baseado no ambiente
 * - STAGING: Opus 4.5 (mÃ¡xima qualidade para testes e desenvolvimento)
 * - PRODUCTION: Sonnet 4.5 (melhor custo-benefÃ­cio)
 * - DEVELOPMENT: Sonnet 4.5 (padrÃ£o)
 */
function getDefaultModel() {
  const env = process.env.NODE_ENV?.toLowerCase() || 'development';
  const forceModel = process.env.DEFAULT_AI_MODEL;

  // Se houver modelo forÃ§ado via env var, usar ele
  if (forceModel) {
    console.log(`ğŸ¯ Usando modelo forÃ§ado via DEFAULT_AI_MODEL: ${forceModel}`);
    return forceModel;
  }

  // STAGING: Usar Opus 4.5 (mÃ¡xima qualidade)
  if (env === 'staging' || process.env.RENDER_SERVICE_NAME?.includes('staging')) {
    console.log('ğŸš€ STAGING detectado: usando Claude Opus 4.5 (mÃ¡xima qualidade)');
    return 'us.anthropic.claude-opus-4-5-20251101-v1:0';
  }

  // PRODUCTION e DEVELOPMENT: Usar Sonnet 4.5 (custo-benefÃ­cio)
  console.log(`ğŸ“Š ${env.toUpperCase()}: usando Claude Sonnet 4.5 (custo-benefÃ­cio)`);
  return 'us.anthropic.claude-sonnet-4-5-20250929-v1:0';
}

const CONFIG = {
  region: process.env.AWS_REGION || 'us-west-2',
  defaultModel: getDefaultModel(),
  maxTokens: 64000,  // ğŸ¯ LIMITE PADRÃƒO: 64K tokens (~192K chars) - LIMITE REAL DO CLAUDE SONNET 4.5
  maxTokensLongForm: 64000,  // ğŸ“„ LIMITE DOCUMENTOS GRANDES: 64K tokens (MÃXIMO do modelo)
  maxTokensAbsolute: 64000,  // ğŸš€ MÃXIMO ABSOLUTO: 64K tokens (limite do AWS Bedrock Claude)
  temperature: 0.7,
  autoModelSelection: true,  // Habilitar seleÃ§Ã£o automÃ¡tica de modelo
  maxContextTokens: 200000,  // Limite de contexto de entrada (200k tokens - Sonnet/Opus 4.5)
  artifactStreamingThreshold: 8192,  // ğŸ“¦ THRESHOLD: >8KB = acumular e enviar de uma vez (evita QUIC error)
  artifactProgressiveMaxSize: 32768  // ğŸ“¦ Se <32KB = streaming progressivo OK
};

// Modelos disponÃ­veis organizados por provedor
export const MODELOS_BEDROCK = {
  amazon: {
    'nova-premier': 'amazon.nova-premier-v1:0',
    'nova-pro': 'amazon.nova-pro-v1:0',
    'nova-lite': 'amazon.nova-lite-v1:0',
    'nova-micro': 'amazon.nova-micro-v1:0',
    'titan-text': 'amazon.titan-text-express-v1'
  },
  anthropic: {
    'claude-opus-4.5': 'us.anthropic.claude-opus-4-5-20251101-v1:0',
    'claude-opus-4': 'us.anthropic.claude-opus-4-20250514-v1:0',
    'claude-sonnet-4.5': 'us.anthropic.claude-sonnet-4-5-20250929-v1:0',
    'claude-sonnet-4': 'us.anthropic.claude-sonnet-4-20250514-v1:0',
    'claude-haiku-4.5': 'us.anthropic.claude-haiku-4-5-20251001-v1:0',
    'claude-3.7-sonnet': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',
    'claude-3.5-sonnet': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',
    'claude-3.5-haiku': 'us.anthropic.claude-3-5-haiku-20241022-v1:0',
    'claude-3-opus': 'us.anthropic.claude-3-opus-20240229-v1:0',
    'claude-3-sonnet': 'anthropic.claude-3-sonnet-20240229-v1:0',
    'claude-3-haiku': 'anthropic.claude-3-haiku-20240307-v1:0'
  },
  meta: {
    'llama-4-scout': 'meta.llama4-scout-17b-instruct-v1:0',
    'llama-4-maverick': 'meta.llama4-maverick-17b-instruct-v1:0',
    'llama-3.3-70b': 'meta.llama3-3-70b-instruct-v1:0',
    'llama-3.2-90b': 'meta.llama3-2-90b-instruct-v1:0',
    'llama-3.2-11b': 'meta.llama3-2-11b-instruct-v1:0',
    'llama-3.1-70b': 'meta.llama3-1-70b-instruct-v1:0',
    'llama-3.1-8b': 'meta.llama3-1-8b-instruct-v1:0'
  },
  mistral: {
    'mistral-large-3': 'mistral.mistral-large-3-675b-instruct',
    'pixtral-large': 'mistral.pixtral-large-2502-v1:0',
    'ministral-14b': 'mistral.ministral-3-14b-instruct',
    'ministral-8b': 'mistral.ministral-3-8b-instruct'
  },
  deepseek: {
    'r1': 'deepseek.r1-v1:0',
    'deepseek-r1': 'deepseek.r1-v1:0'
  },
  cohere: {
    'command-r-plus': 'cohere.command-r-plus-v1:0',
    'command-r': 'cohere.command-r-v1:0'
  }
};

// Inference Profiles para modelos que requerem
export const INFERENCE_PROFILES = {
  // Meta Llama
  'meta.llama3-3-70b-instruct-v1:0': 'us.meta.llama3-3-70b-instruct-v1:0',
  'meta.llama3-2-90b-instruct-v1:0': 'us.meta.llama3-2-90b-instruct-v1:0',
  'meta.llama3-2-11b-instruct-v1:0': 'us.meta.llama3-2-11b-instruct-v1:0',
  'meta.llama3-1-70b-instruct-v1:0': 'us.meta.llama3-1-70b-instruct-v1:0',
  'meta.llama3-1-8b-instruct-v1:0': 'us.meta.llama3-1-8b-instruct-v1:0',
  'meta.llama4-scout-17b-instruct-v1:0': 'us.meta.llama4-scout-17b-instruct-v1:0',
  'meta.llama4-maverick-17b-instruct-v1:0': 'us.meta.llama4-maverick-17b-instruct-v1:0',

  // Anthropic Claude (modelos que requerem inference profile)
  'anthropic.claude-3-opus-20240229-v1:0': 'us.anthropic.claude-3-opus-20240229-v1:0',
  'anthropic.claude-3-5-haiku-20241022-v1:0': 'us.anthropic.claude-3-5-haiku-20241022-v1:0',
  'anthropic.claude-3-5-sonnet-20241022-v2:0': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',
  'anthropic.claude-3-7-sonnet-20250219-v1:0': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',
  'anthropic.claude-sonnet-4-20250514-v1:0': 'us.anthropic.claude-sonnet-4-20250514-v1:0',
  'anthropic.claude-sonnet-4-5-20250929-v1:0': 'us.anthropic.claude-sonnet-4-5-20250929-v1:0',
  'anthropic.claude-opus-4-20250514-v1:0': 'us.anthropic.claude-opus-4-20250514-v1:0',
  'anthropic.claude-opus-4-5-20251101-v1:0': 'us.anthropic.claude-opus-4-5-20251101-v1:0',
  'anthropic.claude-haiku-4-5-20251001-v1:0': 'us.anthropic.claude-haiku-4-5-20251001-v1:0',

  // Mapeamentos simplificados para compatibilidade com frontend (usando hÃ­fens)
  'claude-opus-4-5': 'us.anthropic.claude-opus-4-5-20251101-v1:0',
  'claude-opus-4': 'us.anthropic.claude-opus-4-20250514-v1:0',
  'claude-sonnet-4-5': 'us.anthropic.claude-sonnet-4-5-20250929-v1:0',
  'claude-sonnet-4': 'us.anthropic.claude-sonnet-4-20250514-v1:0',
  'claude-haiku-4-5': 'us.anthropic.claude-haiku-4-5-20251001-v1:0',

  // Amazon Nova
  'amazon.nova-premier-v1:0': 'us.amazon.nova-premier-v1:0',
  'amazon.nova-pro-v1:0': 'us.amazon.nova-pro-v1:0',
  'amazon.nova-lite-v1:0': 'us.amazon.nova-lite-v1:0',
  'amazon.nova-micro-v1:0': 'us.amazon.nova-micro-v1:0',

  // DeepSeek
  'deepseek.r1-v1:0': 'us.deepseek.r1-v1:0',

  // Mistral
  'mistral.mistral-large-3-675b-instruct': 'us.mistral.mistral-large-3-675b-instruct',
  'mistral.pixtral-large-2502-v1:0': 'us.mistral.pixtral-large-2502-v1:0'
};

// ============================================================
// CLIENTE BEDROCK
// ============================================================

let runtimeClient = null;
let managementClient = null;

function getBedrockRuntimeClient() {
  if (!runtimeClient) {
    runtimeClient = new BedrockRuntimeClient({
      region: CONFIG.region,
      requestHandler: {
        requestTimeout: 300000  // 300 segundos (5 min) - necessÃ¡rio para peÃ§as muito grandes (25-30 pÃ¡ginas)
      }
    });
  }
  return runtimeClient;
}

function getBedrockManagementClient() {
  if (!managementClient) {
    managementClient = new BedrockClient({
      region: CONFIG.region,
      requestHandler: {
        requestTimeout: 300000  // 300 segundos (5 min) - necessÃ¡rio para peÃ§as muito grandes (25-30 pÃ¡ginas)
      }
    });
  }
  return managementClient;
}

// ============================================================
// FUNÃ‡Ã•ES PRINCIPAIS
// ============================================================

/**
 * Envia mensagem para modelo Bedrock usando a API Converse
 * @param {string} prompt - Mensagem do usuÃ¡rio
 * @param {object} options - OpÃ§Ãµes de configuraÃ§Ã£o
 * @returns {Promise<object>} Resposta do modelo
 */
export async function conversar(prompt, options = {}) {
  const {
    modelo = CONFIG.defaultModel,
    systemPrompt = null,
    historico = [],
    maxTokens = CONFIG.maxTokens,
    temperature = CONFIG.temperature,
    topP = 0.9,
    enableTools = true,  // â† NOVO: habilitar tools por padrÃ£o
    kbContext = '',  // â† NOVO: contexto do KB para cÃ¡lculo de tokens
    enableCache = true,  // â† v2.7.0: Enable cache (default true)
    cacheType = 'simple'  // â† v2.7.0: Cache type (simple, jurisprudence, legislation, templates)
  } = options;

  // ğŸ”¥ v2.7.0: CACHE CHECK (10-50x faster on hits)
  if (enableCache && !enableTools) { // Only cache non-tool responses
    const cache = getCache();
    const cacheKey = cache.generateKey(prompt, modelo, { temperature, maxTokens });

    const cached = await cache.get(cacheKey, cacheType);
    if (cached) {
      console.log(`ğŸ’¾ [Cache HIT] Returning cached response`);
      return {
        ...cached,
        fromCache: true
      };
    }
  }

  const client = getBedrockRuntimeClient();

  // ğŸ”¥ TRUNCAR HISTÃ“RICO PARA EVITAR "Input is too long"
  // Calcular limite baseado no modelo especÃ­fico (cada modelo tem limite diferente)
  const safeLimit = contextManager.getSafeContextLimit(modelo); // 70% do limite do modelo

  const truncatedHistory = contextManager.truncateHistory(
    historico,
    safeLimit,  // Limite seguro baseado no modelo especÃ­fico
    kbContext,
    prompt
  );

  // ğŸ”¥ CONCATENAR KB CONTEXT DEPOIS DO TRUNCAMENTO
  const finalPrompt = kbContext ? prompt + '\n\n' + kbContext : prompt;

  // Construir mensagens iniciais
  const initialMessages = [
    ...truncatedHistory.map(msg => ({
      role: msg.role,
      content: [{ text: msg.content }]
    })),
    {
      role: 'user',
      content: [{ text: finalPrompt }]  // ğŸ”¥ Usar prompt final com KB
    }
  ];

  // Configurar inferÃªncia
  const modeloId = INFERENCE_PROFILES[modelo] || modelo;
  const isClaude45 = modeloId.includes('claude-haiku-4-5') ||
                     modeloId.includes('claude-sonnet-4-5') ||
                     modeloId.includes('claude-opus-4-5');

  const inferenceConfig = isClaude45
    ? { maxTokens }
    : { maxTokens, temperature, topP };

  try {
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // LOOP DE TOOL USE
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    const conversationId = `conv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

    // Inicializar guardrails para esta conversaÃ§Ã£o
    loopGuardrails.initConversation(conversationId);

    let currentMessages = initialMessages;
    let loopCount = 0;
    const MAX_LOOPS = 100;  // Aumentado de 10 para 100 para anÃ¡lises exaustivas (BACKSPEC BETA)
    let totalTokensUsed = { input: 0, output: 0 };
    const toolsUsed = [];

    while (loopCount < MAX_LOOPS) {
      // Montar comando
      const commandParams = {
        modelId: INFERENCE_PROFILES[modelo] || modelo,
        messages: currentMessages,
        inferenceConfig
      };

      // Adicionar system prompt with caching (v2.7.0)
      if (systemPrompt) {
        const usePromptCaching = options.enablePromptCaching !== false; // Default true
        const systemPromptLength = systemPrompt.length;
        const shouldCache = usePromptCaching && systemPromptLength > 1024; // Only cache if >1024 chars

        if (shouldCache) {
          // Use prompt caching for large system prompts (90% cost reduction)
          commandParams.system = [{
            text: systemPrompt,
            cacheControl: { type: 'ephemeral' } // Cache for 5 minutes
          }];
          console.log(`ğŸ’° [Prompt Caching] ENABLED for system prompt (${systemPromptLength} chars)`);
        } else {
          commandParams.system = [{ text: systemPrompt }];
        }
      }

      // ğŸ”¥ v2.7.0: KB Context with Prompt Caching
      if (kbContext && kbContext.length > 2048) {
        // Add KB as separate system message with caching
        if (!commandParams.system) {
          commandParams.system = [];
        }
        commandParams.system.push({
          text: `# Knowledge Base Context\n\n${kbContext}`,
          cacheControl: { type: 'ephemeral' } // Cache KB for 5 minutes
        });
        console.log(`ğŸ’° [Prompt Caching] ENABLED for KB context (${kbContext.length} chars)`);
      }

      // âœ… NOVO v2.8.0: Multi-model compatibility - verificar se modelo suporta tool use
      const actualModelId = INFERENCE_PROFILES[modelo] || modelo;
      const toolsEnabled = shouldEnableTools(actualModelId, enableTools);

      if (toolsEnabled) {
        commandParams.toolConfig = { tools: BEDROCK_TOOLS };
      } else if (loopCount === 0 && enableTools && !getModelCapabilities(actualModelId).toolUse) {
        // Log apenas na primeira iteraÃ§Ã£o se modelo nÃ£o suporta tools
        console.log(`âš ï¸ [Converse] Tools DISABLED - modelo nÃ£o suporta tool use`);
      }

      const command = new ConverseCommand(commandParams);

      // Envolver com resilientInvoke: Circuit Breaker + Fallback + Retry + Bottleneck
      const response = await resilientInvoke(client, command, {
        modelId: commandParams.modelId,
        operation: 'converse',
        requestId: options.conversationId || `conv_${Date.now()}`,
        loopIteration: loopCount,
        enableFallback: true,
        enableCircuitBreaker: true
      });

      // Acumular uso de tokens
      totalTokensUsed.input += response.usage.inputTokens;
      totalTokensUsed.output += response.usage.outputTokens;

      // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      // VERIFICAR SE MODELO QUER USAR TOOL
      // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (response.stopReason === 'tool_use') {
        const toolUses = response.output.message.content.filter(c => c.toolUse);

        // Adicionar mensagem do assistente (com tool_use)
        currentMessages.push(response.output.message);

        // Executar cada tool solicitada
        const toolResults = [];
        for (const toolUseBlock of toolUses) {
          const { toolUseId, name, input } = toolUseBlock.toolUse;

          // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          // GUARDRAIL: Verificar antes de executar tool
          // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          const guardrailCheck = loopGuardrails.trackToolUse(conversationId, name);

          if (!guardrailCheck.allowed) {
            console.error(`ğŸ›¡ï¸ [Guardrail] ${guardrailCheck.reason.toUpperCase()} - Bloqueando execuÃ§Ã£o`);

            // Adicionar mensagem de erro como resultado da tool
            toolResults.push({
              toolResult: {
                toolUseId,
                content: [{
                  text: `[GUARDRAIL ATIVADO] ${guardrailCheck.message}`
                }]
              }
            });

            // ForÃ§ar fim do loop
            loopCount = MAX_LOOPS;
            break;
          }

          console.log(`ğŸ”§ [Tool Use] ${name}:`, JSON.stringify(input, null, 2));
          toolsUsed.push({ name, input });

          try {
            const result = await executeTool(name, input);

            toolResults.push({
              toolResult: {
                toolUseId,
                content: [{
                  text: result.success ? result.content : `Erro: ${result.error || result.content}`
                }]
              }
            });

            console.log(`âœ… [Tool Use] ${name} executada com sucesso`);
          } catch (error) {
            console.error(`âŒ [Tool Use] Erro ao executar ${name}:`, error);

            toolResults.push({
              toolResult: {
                toolUseId,
                content: [{ text: `Erro ao executar tool: ${error.message}` }]
              }
            });
          }
        }

        // Adicionar resultados das tools como nova mensagem do user
        currentMessages.push({
          role: 'user',
          content: toolResults
        });

        loopCount++;
        continue;  // Fazer nova chamada com os resultados
      }

      // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      // MODELO NÃƒO QUER MAIS USAR TOOLS - RETORNAR RESPOSTA
      // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      const content = response.output.message.content[0];
      let resposta = '';
      let raciocinio = null;

      if (content.text) {
        // Resposta normal (Claude, Nova, Llama, etc)
        resposta = content.text;
      } else if (content.reasoningContent) {
        // Modelo de raciocÃ­nio (DeepSeek R1)
        raciocinio = content.reasoningContent.reasoningText?.text || '';
        resposta = raciocinio;
      }

      // Cleanup guardrails apÃ³s conversaÃ§Ã£o bem-sucedida
      loopGuardrails.cleanupConversation(conversationId);

      const resultadoFinal = {
        sucesso: true,
        resposta,
        raciocinio,
        modelo,
        uso: {
          tokensEntrada: totalTokensUsed.input,
          tokensSaida: totalTokensUsed.output,
          tokensTotal: totalTokensUsed.input + totalTokensUsed.output
        },
        toolsUsadas: toolsUsed.length > 0 ? toolsUsed : undefined,  // â† NOVO
        latencia: response.metrics?.latencyMs || null,
        motivoParada: response.stopReason,
        guardrailStats: loopGuardrails.getStats(conversationId)  // â† NOVO: stats do guardrail
      };

      // ğŸ”¥ v2.7.0: CACHE STORE (only if tools weren't used)
      if (enableCache && toolsUsed.length === 0) {
        const cache = getCache();
        const cacheKey = cache.generateKey(prompt, modelo, { temperature, maxTokens });
        await cache.set(cacheKey, resultadoFinal, cacheType);
        console.log(`ğŸ’¾ [Cache SET] Stored response in cache (type: ${cacheType})`);
      }

      return resultadoFinal;
    }

    // Se chegou ao limite de loops
    throw new Error(`Limite de tool use loops atingido (${MAX_LOOPS} iteraÃ§Ãµes)`);

  } catch (error) {
    console.error('âŒ [Bedrock] Erro na conversaÃ§Ã£o:', error);

    // Preservar propriedades do erro (statusCode, retryAfter, etc) no objeto de retorno
    return {
      sucesso: false,
      erro: error?.message || 'Erro ao processar conversa',
      statusCode: Number.isInteger(error?.statusCode)
        ? error.statusCode
        : (Number.isInteger(error?.status) ? error.status : undefined),
      retryAfter: error?.retryAfter,
      // Campos auxiliares para debug
      errorName: error?.name,
      errorCode: error?.code,
      modelo
    };
  }
}

/**
 * Envia mensagem com streaming
 * @param {string} prompt - Mensagem do usuÃ¡rio
 * @param {function} onChunk - Callback para cada chunk
 * @param {object} options - OpÃ§Ãµes de configuraÃ§Ã£o
 */
/**
 * Detecta se o texto parece ser inÃ­cio de um documento estruturado
 * VERSÃƒO INTELIGENTE - Detecta mÃºltiplos padrÃµes de documentos
 *
 * @param {string} text - Texto acumulado atÃ© agora
 * @param {object} context - Contexto adicional (usouFerramentas, etc.)
 * @returns {object|null} - { type, title } se for documento, null caso contrÃ¡rio
 */
function detectDocumentStart(text, context = {}) {
  const trimmed = text.trim();
  const { usouFerramentas = false } = context;

  // âš¡ DETECÃ‡ÃƒO INSTANTÃ‚NEA: Detectar "# " no inÃ­cio (Markdown heading)
  if (trimmed.startsWith('#')) {
    if (trimmed.length >= 3) {
      const titleMatch = trimmed.match(/^#\s+(.+)/m);
      if (titleMatch) {
        const title = titleMatch[1].trim();
        if (title.length >= 5) {
          return { type: 'document', title };
        }
        return { type: 'document', title: title + '...' };
      }
    }
  }

  // ğŸ¯ DETECÃ‡ÃƒO INTELIGENTE #1: MÃºltiplos headings (documento estruturado)
  // Se tem 2+ headings markdown (##, ###, etc.) = documento
  const headingCount = (trimmed.match(/^#{1,6}\s+/gm) || []).length;
  if (headingCount >= 2) {
    const firstHeading = trimmed.match(/^#\s+(.+)/m);
    const title = firstHeading ? firstHeading[1].trim() : 'Documento Estruturado';
    console.log(`ğŸ¯ [Smart Detection] MÃºltiplos headings (${headingCount}) = Documento`);
    return { type: 'document', title };
  }

  // ğŸ¯ DETECÃ‡ÃƒO INTELIGENTE #2: Resposta longa estruturada (>800 chars com parÃ¡grafos)
  // Se passou de 800 chars E tem estrutura (3+ parÃ¡grafos separados) = documento
  if (trimmed.length > 800) {
    const paragraphs = trimmed.split(/\n\n+/).filter(p => p.trim().length > 50);
    if (paragraphs.length >= 3) {
      // Tentar extrair tÃ­tulo do primeiro parÃ¡grafo ou linha
      const firstLine = trimmed.split('\n')[0].trim();
      const title = firstLine.length > 10 && firstLine.length < 100
        ? firstLine.replace(/^[#*_]+\s*/, '')
        : 'AnÃ¡lise Estruturada';
      console.log(`ğŸ¯ [Smart Detection] Resposta longa estruturada (${trimmed.length} chars, ${paragraphs.length} Â§) = Documento`);
      return { type: 'document', title };
    }
  }

  // ğŸ¯ DETECÃ‡ÃƒO INTELIGENTE #3: Usou ferramentas de pesquisa = anÃ¡lise estruturada
  // Se usou pesquisa de jurisprudÃªncia e estÃ¡ gerando resposta longa = anÃ¡lise
  if (usouFerramentas && trimmed.length > 500) {
    console.log(`ğŸ¯ [Smart Detection] Usou ferramentas + resposta longa (${trimmed.length} chars) = AnÃ¡lise`);
    return { type: 'document', title: 'AnÃ¡lise com JurisprudÃªncia' };
  }

  // ğŸ“‹ PADRÃ•ES CLÃSSICOS: Palavras-chave especÃ­ficas
  const patterns = [
    { regex: /^#\s+([A-ZÃ€-Ãš][^\n]*)/m, type: 'document', titleGroup: 1 },
    { regex: /^EXCELENTÃSSIM[OA]/i, type: 'document', title: 'PetiÃ§Ã£o' },
    { regex: /^MEMORIAL/i, type: 'document', title: 'Memorial' },
    { regex: /^CONTRATO/i, type: 'document', title: 'Contrato' },
    { regex: /^PARECER/i, type: 'document', title: 'Parecer' },
    { regex: /^SENTENÃ‡A/i, type: 'document', title: 'SentenÃ§a' },
    { regex: /^ACÃ“RDÃƒO/i, type: 'document', title: 'AcÃ³rdÃ£o' },
    { regex: /^RECURSO/i, type: 'document', title: 'Recurso' },
    { regex: /^AGRAVO/i, type: 'document', title: 'Agravo' },
    { regex: /^APELAÃ‡ÃƒO/i, type: 'document', title: 'ApelaÃ§Ã£o' },
    { regex: /^ANÃLISE/i, type: 'document', title: 'AnÃ¡lise' },
    { regex: /^RELATÃ“RIO/i, type: 'document', title: 'RelatÃ³rio' },
    { regex: /^RESUMO/i, type: 'document', title: 'Resumo' },
    { regex: /^PESQUISA/i, type: 'document', title: 'Pesquisa' },
  ];

  for (const pattern of patterns) {
    const match = trimmed.match(pattern.regex);
    if (match) {
      const title = pattern.titleGroup ? match[pattern.titleGroup].trim() : pattern.title;
      return { type: pattern.type, title };
    }
  }

  return null;
}

export async function conversarStream(prompt, onChunk, options = {}) {
  const {
    modelo = CONFIG.defaultModel,
    systemPrompt = null,
    historico = [],
    maxTokens: requestedMaxTokens = CONFIG.maxTokens,
    temperature = CONFIG.temperature,
    kbContext = '',  // â† NOVO: contexto do KB para cÃ¡lculo de tokens
    enableTools = true  // âœ… NOVO: Habilitar ferramentas por padrÃ£o (jurisprudÃªncia, KB, CNJ)
  } = options;

  // ğŸ›¡ï¸ SEGURANÃ‡A: Limitar maxTokens ao mÃ¡ximo absoluto
  // Se nÃ£o especificado explicitamente, usar limite padrÃ£o (16K)
  // Se especificado, respeitar mas nÃ£o ultrapassar limite absoluto (200K)
  const maxTokens = Math.min(requestedMaxTokens, CONFIG.maxTokensAbsolute);

  if (requestedMaxTokens > CONFIG.maxTokensAbsolute) {
    console.warn(`âš ï¸ [conversarStream] maxTokens requested (${requestedMaxTokens}) exceeds absolute limit, capping at ${CONFIG.maxTokensAbsolute}`);
  }

  console.log('ğŸš€ [conversarStream] STARTED with:', {
    promptLength: prompt?.length || 0,
    promptPreview: prompt?.substring(0, 150),
    hasSystemPrompt: !!systemPrompt,
    systemPromptLength: systemPrompt?.length || 0,
    systemPromptPreview: systemPrompt?.substring(0, 150),
    modelo,
    historicoLength: historico?.length || 0,
    kbContextLength: kbContext?.length || 0,
    enableTools,
    maxTokensRequested: requestedMaxTokens,
    maxTokensUsed: maxTokens,
    temperature,
    hasOnChunkCallback: typeof onChunk === 'function'
  });

  if (!systemPrompt) {
    console.error('âŒ [conversarStream] CRITICAL: systemPrompt is NULL! Model WILL NOT respond correctly!');
  }

  const client = getBedrockRuntimeClient();

  // ğŸ”¥ TRUNCAR HISTÃ“RICO PARA EVITAR "Input is too long"
  // Calcular limite baseado no modelo especÃ­fico (cada modelo tem limite diferente)
  const safeLimit = contextManager.getSafeContextLimit(modelo); // 70% do limite do modelo

  const truncatedHistory = contextManager.truncateHistory(
    historico,
    safeLimit,  // Limite seguro baseado no modelo especÃ­fico
    kbContext,
    prompt
  );

  // Log apenas em desenvolvimento (removido debug verboso para producao)

  // ğŸ”¥ CONCATENAR KB CONTEXT DEPOIS DO TRUNCAMENTO
  const finalPrompt = kbContext ? prompt + '\n\n' + kbContext : prompt;

  // Calcular tamanho total do histÃ³rico em caracteres (para debug de max_tokens)
  const totalHistoryChars = truncatedHistory.reduce((sum, msg) => sum + msg.content.length, 0);
  const estimatedTokens = Math.ceil((systemPrompt?.length || 0) / 4) + Math.ceil(totalHistoryChars / 4) + Math.ceil(finalPrompt.length / 4);

  console.log(`ğŸ“ [conversarStream] Final prompt constructed:`, {
    originalPromptLength: prompt.length,
    kbContextLength: kbContext.length,
    finalPromptLength: finalPrompt.length,
    truncatedHistoryLength: truncatedHistory.length,
    totalHistoryChars,
    estimatedInputTokens: estimatedTokens,
    maxTokensOutput: maxTokens
  });

  if (estimatedTokens > 150000) {
    console.warn(`âš ï¸ [conversarStream] WARNING: Estimated input tokens (${estimatedTokens}) is very high! May hit max_tokens.`);
  }

  const messages = [
    ...truncatedHistory.map(msg => ({
      role: msg.role,
      content: [{ text: msg.content }]
    })),
    {
      role: 'user',
      content: [{ text: finalPrompt }]  // ğŸ”¥ Usar prompt final com KB
    }
  ];

  console.log(`ğŸ“¨ [conversarStream] Messages array prepared with ${messages.length} messages`);


  const commandParams = {
    modelId: INFERENCE_PROFILES[modelo] || modelo,
    messages,
    inferenceConfig: { maxTokens, temperature }
  };

  // Adicionar system prompt with caching (v2.7.0)
  if (systemPrompt) {
    const usePromptCaching = options.enablePromptCaching !== false; // Default true
    const shouldCache = usePromptCaching && systemPrompt.length > 1024;

    if (shouldCache) {
      commandParams.system = [{
        text: systemPrompt,
        cacheControl: { type: 'ephemeral' }
      }];
      console.log(`ğŸ’° [Stream] Prompt Caching ENABLED (${systemPrompt.length} chars)`);
    } else {
      commandParams.system = [{ text: systemPrompt }];
    }
  }

  // KB Context caching
  if (kbContext && kbContext.length > 2048) {
    if (!commandParams.system) {
      commandParams.system = [];
    }
    commandParams.system.push({
      text: `# Knowledge Base Context\n\n${kbContext}`,
      cacheControl: { type: 'ephemeral' }
    });
    console.log(`ğŸ’° [Stream] KB Cache ENABLED (${kbContext.length} chars)`);
  }

  // âœ… NOVO v2.7.2: Adicionar ferramentas (jurisprudÃªncia, KB, CNJ, sÃºmulas)
  // âœ… NOVO v2.8.0: Multi-model compatibility - verificar se modelo suporta tool use
  const actualModelId = INFERENCE_PROFILES[modelo] || modelo;
  const modelCapabilities = getModelCapabilities(actualModelId);
  const toolsEnabled = shouldEnableTools(actualModelId, enableTools);

  if (toolsEnabled) {
    commandParams.toolConfig = { tools: BEDROCK_TOOLS };
    console.log(`ğŸ”§ [Stream] Tools ENABLED (${BEDROCK_TOOLS.length} ferramentas | ${modelCapabilities.provider})`);
  } else {
    if (enableTools && !modelCapabilities.toolUse) {
      // UsuÃ¡rio queria tools, mas modelo nÃ£o suporta
      console.log(`âš ï¸ [Stream] Tools DISABLED - modelo ${modelCapabilities.provider} nÃ£o suporta tool use`);
      console.log(`ğŸ’¡ [Stream] Use Claude Sonnet/Opus ou Amazon Nova Pro para busca automÃ¡tica`);
    } else {
      console.log(`ğŸ”§ [Stream] Tools DISABLED (desabilitado pelo usuÃ¡rio)`);
    }
  }

  try {
    // âœ… NOVO v2.8.0: Informar usuÃ¡rio se tools estÃ£o indisponÃ­veis devido ao modelo
    if (enableTools && !modelCapabilities.toolUse) {
      const warningMessage = getToolsUnavailableMessage(actualModelId);
      if (warningMessage) {
        onChunk(warningMessage + '\n\n');
      }
    }

    let currentMessages = messages;
    let loopCount = 0;
    const MAX_TOOL_LOOPS = 3; // âœ… v2.9.2: 2 buscas + 1 apresentaÃ§Ã£o forÃ§ada (balance latÃªncia/completude)
    let hasJurisprudenceResults = false;

    // v2.9.0: Metricas de performance SSE
    const streamStartTime = Date.now();
    metrics.incrementTotalRequests();

    // ğŸ”§ FIX: Estado de artifacts FORA do loop para preservar entre tool calls
    // Se resetar a cada loop, documentos com ferramentas geram mÃºltiplos artifacts!
    let isStreamingArtifact = false;
    let artifactWillStreamProgressively = false;
    let artifactMetadata = null;
    let artifactContent = '';
    let artifactId = null;

    while (loopCount < MAX_TOOL_LOOPS) {
      // ğŸ”„ v2.9.0: Logging melhorado para rastreamento de loops
      console.log(`ğŸ”„ [Loop ${loopCount + 1}/${MAX_TOOL_LOOPS}] Processing tool results...`);

      // ğŸ”§ FIX: Remover tools no Ãºltimo loop para forÃ§ar apresentaÃ§Ã£o
      const isLastLoop = loopCount >= MAX_TOOL_LOOPS - 1;
      const currentParams = { ...commandParams, messages: currentMessages };

      if (isLastLoop) {
        // Remover toolConfig no Ãºltimo loop - forÃ§a Claude a apresentar o que tem
        delete currentParams.toolConfig;
        console.log(`âš ï¸ [MAX_LOOPS REACHED] TOOLS DISABLED - Forcing final presentation (loopCount=${loopCount})`);
      }

      console.log(`ğŸ“¤ [Loop ${loopCount}] Sending request to Bedrock with ${currentMessages.length} messages (tools: ${!!currentParams.toolConfig})`);

      const command = new ConverseStreamCommand(currentParams);

      let response;
      try {
        response = await retryAwsCommand(client, command, { modelId: commandParams.modelId, operation: 'converse_stream' });
        console.log(`ğŸ“¥ [Loop ${loopCount}] Bedrock response received, starting to process stream...`);
      } catch (bedrockError) {
        console.error(`âŒ [Loop ${loopCount}] Bedrock API error:`, {
          error: bedrockError.message,
          name: bedrockError.name,
          code: bedrockError.code,
          statusCode: bedrockError.statusCode
        });
        throw bedrockError;
      }

      let textoCompleto = '';
      let stopReason = null;
      let toolUseData = [];
      let currentToolUse = null;
      let eventCount = 0;

      // ğŸ”§ FIX: VariÃ¡veis de artifact agora sÃ£o preservadas entre loops (movidas para fora)
      // Apenas resetar usouFerramentas baseado no loop atual
      let usouFerramentas = toolUseData.length > 0; // Rastrear se usou ferramentas neste loop

      console.log(`ğŸ”„ [Stream Loop ${loopCount}] Starting to process Bedrock stream...`);

      // Processar stream de eventos
      let lastLogTime = Date.now();
      for await (const event of response.stream) {
        eventCount++;

        // ğŸ• Log periÃ³dico de progresso (a cada 5 segundos OU evento importante)
        const now = Date.now();
        const shouldLogProgress = (now - lastLogTime) > 5000;
        const isImportantEvent = eventCount <= 5 || eventCount % 100 === 0;

        if (shouldLogProgress || isImportantEvent) {
          console.log(`ğŸ“¦ [Stream Loop ${loopCount}] Event #${eventCount} (${Math.round((now - lastLogTime) / 1000)}s since last log):`, {
            keys: Object.keys(event),
            hasText: !!event.contentBlockDelta?.delta?.text,
            hasToolUse: !!event.contentBlockStart?.start?.toolUse || !!event.contentBlockDelta?.delta?.toolUse,
            stopReason: event.messageStop?.stopReason,
            textoCompletoLength: textoCompleto.length,
            toolInputLength: currentToolUse?.input?.length || 0
          });
          lastLogTime = now;
        }

        // Texto sendo gerado
        if (event.contentBlockDelta?.delta?.text) {
          const chunk = event.contentBlockDelta.delta.text;
          textoCompleto += chunk;

          console.log(`ğŸ“ [Stream Loop ${loopCount}] Text chunk received (${chunk.length} chars)`);

          // ğŸ¨ DETECÃ‡ÃƒO INTELIGENTE: Verificar se estÃ¡ iniciando um documento estruturado
          // âœ… OTIMIZAÃ‡ÃƒO: Detectar a partir de 5 chars
          // âœ… SMART: Detecta respostas longas estruturadas e uso de ferramentas
          // Janela de detecÃ§Ã£o: 5-1500 chars (expandida para detecÃ§Ã£o inteligente)
          if (!isStreamingArtifact && textoCompleto.length >= 5 && textoCompleto.length <= 1500) {
            const detection = detectDocumentStart(textoCompleto, { usouFerramentas });
            if (detection) {
              console.log(`ğŸ¨ [Smart Artifact Detection] Documento detectado: "${detection.title}" (${detection.type}) em ${textoCompleto.length} chars`);

              isStreamingArtifact = true;
              artifactId = `artifact_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
              artifactMetadata = {
                id: artifactId,
                title: detection.title,
                type: detection.type,
                language: 'markdown',
                createdAt: new Date().toISOString()
              };
              artifactContent = textoCompleto; // Incluir o que jÃ¡ foi gerado

              // ğŸš€ DECISÃƒO: Streaming progressivo ou acumulaÃ§Ã£o?
              // Se tÃ­tulo indica documento grande (anÃ¡lise, memorial, parecer) = acumular
              const isLargeDocument = /anÃ¡lise|memorial|parecer|petiÃ§Ã£o|acÃ³rdÃ£o|sentenÃ§a|completa|pormenorizada|detalhada/i.test(detection.title);

              if (isLargeDocument) {
                console.log(`   ğŸ“„ Documento GRANDE detectado: acumulando para envio Ãºnico (evita QUIC error)`);
                // NÃƒO enviar artifact_start - vai enviar artifact_complete no final
                artifactWillStreamProgressively = false;
              } else {
                console.log(`   âš¡ Documento pequeno: streaming progressivo habilitado`);
                artifactWillStreamProgressively = true;

                // Enviar evento de inÃ­cio de artifact IMEDIATAMENTE
                try {
                  onChunk({
                    __artifact_start: artifactMetadata
                  });
                  console.log(`   ğŸ“¤ artifact_start enviado: ${detection.title}`);
                } catch (err) {
                  console.error('[Artifact Start] Erro ao enviar:', err.message);
                }
              }
            }
          }

          // âœ… ROTEAMENTO: Enviar chunk para artifact ou chat normal
          try {
            if (isStreamingArtifact) {
              // Acumular conteÃºdo do artifact
              artifactContent += chunk;

              // ğŸš€ DECISÃƒO: Streaming progressivo ou acumulaÃ§Ã£o?
              if (artifactWillStreamProgressively) {
                // ğŸ“¡ STREAMING PROGRESSIVO: Enviar chunks em tempo real (documentos pequenos)
                onChunk({
                  __artifact_chunk: {
                    id: artifactId,
                    content: chunk
                  }
                });
                console.log(`   ğŸ“¤ artifact_chunk enviado (${chunk.length} chars, total: ${artifactContent.length})`);

                // ğŸ›¡ï¸ PROTEÃ‡ÃƒO: Se crescer muito, desabilitar streaming progressivo
                if (artifactContent.length > CONFIG.artifactProgressiveMaxSize) {
                  console.warn(`âš ï¸ [Artifact] Cresceu muito (${artifactContent.length}), desabilitando streaming progressivo`);
                  artifactWillStreamProgressively = false;

                  // Avisar usuÃ¡rio
                  onChunk(`\n\nğŸ“„ **Documento grande em geraÃ§Ã£o... (${Math.round(artifactContent.length / 1024)}KB)**\n\n`);
                }
              } else {
                // ğŸ“¦ MODO ACUMULAÃ‡ÃƒO: NÃ£o enviar chunks, acumular tudo e enviar no final
                // Apenas log silencioso
                if (artifactContent.length % 5000 === 0) {
                  console.log(`   ğŸ“¦ Acumulando artifact: ${Math.round(artifactContent.length / 1024)}KB`);
                }
              }
            } else {
              // Enviar para chat normal
              onChunk(chunk);
              console.log(`   âœ… onChunk() called successfully`);
            }
          } catch (err) {
            console.error('[Bedrock Stream] onChunk falhou:', err.message);
            // Abortar stream se callback falhou (conexÃ£o SSE morreu)
            break;
          }
        }

        // InÃ­cio de tool use
        if (event.contentBlockStart?.start?.toolUse) {
          currentToolUse = {
            toolUseId: event.contentBlockStart.start.toolUse.toolUseId,
            name: event.contentBlockStart.start.toolUse.name,
            input: ''
          };
        }

        // Input da tool sendo streamed
        if (event.contentBlockDelta?.delta?.toolUse) {
          if (currentToolUse) {
            currentToolUse.input += event.contentBlockDelta.delta.toolUse.input || '';

            // ğŸ“Š Progresso de tool use grande: avisar usuÃ¡rio a cada 10KB
            const inputLength = currentToolUse.input.length;
            if (inputLength > 0 && inputLength % 10000 < 100) {
              const sizeKB = Math.round(inputLength / 1024);
              console.log(`â³ [Tool Use Progress] ${currentToolUse.name}: ${sizeKB}KB gerados...`);

              // Enviar aviso ao usuÃ¡rio se > 20KB (documento grande)
              if (inputLength > 20000 && inputLength < 20100) {
                try {
                  onChunk(`\n\nğŸ“„ **Gerando documento grande...** (${sizeKB}KB)\n\n`);
                } catch (err) {
                  console.error('[Tool Progress] Erro ao enviar aviso:', err.message);
                }
              }
            }
          }
        }

        // Fim do tool use block
        if (event.contentBlockStop && currentToolUse) {
          try {
            currentToolUse.input = JSON.parse(currentToolUse.input);
          } catch (e) {
            console.error(`âŒ [Tool Parse] Falha ao parsear JSON do tool "${currentToolUse.name}"`);
            console.error(`   Input recebido (primeiros 500 chars): ${currentToolUse.input?.substring(0, 500)}`);
            console.error(`   Erro: ${e.message}`);

            // ğŸ†˜ FALLBACK: Se for create_artifact com JSON truncado, tentar completar
            if (currentToolUse.name === 'create_artifact' && typeof currentToolUse.input === 'string') {
              console.warn(`âš ï¸ [Tool Parse] Tentando recuperar create_artifact truncado...`);

              try {
                // Contar chaves abertas vs fechadas
                const openBraces = (currentToolUse.input.match(/{/g) || []).length;
                const closeBraces = (currentToolUse.input.match(/}/g) || []).length;
                const missingBraces = openBraces - closeBraces;

                // Se falta fechar, adicionar }}} no final
                if (missingBraces > 0) {
                  const completed = currentToolUse.input + '}'.repeat(missingBraces);
                  currentToolUse.input = JSON.parse(completed);
                  console.log(`   âœ… JSON completado com ${missingBraces} chave(s) faltante(s)`);
                } else {
                  throw new Error('JSON malformado - nÃ£o Ã© apenas chaves faltantes');
                }
              } catch (e2) {
                console.error(`   âŒ NÃ£o foi possÃ­vel recuperar: ${e2.message}`);
                console.error(`   ğŸš« Tool serÃ¡ IGNORADA para evitar ValidationException`);
                currentToolUse = null; // NÃ£o adicionar tool invÃ¡lida
                return; // Sair sem adicionar aos toolUseData
              }
            } else {
              // Se nÃ£o for create_artifact ou nÃ£o for string, ignorar tool
              console.error(`   ğŸš« Tool serÃ¡ IGNORADA (nÃ£o Ã© create_artifact recuperÃ¡vel)`);
              currentToolUse = null;
              return;
            }
          }

          if (currentToolUse) {
            toolUseData.push(currentToolUse);
          }
          currentToolUse = null;
        }

        // âœ… CORREÃ‡ÃƒO: stopReason estÃ¡ em messageStop, nÃ£o em metadata
        if (event.messageStop) {
          stopReason = event.messageStop.stopReason;
        }
      }

      console.log(`ğŸ [Stream Loop ${loopCount}] Stream processing completed:`, {
        eventCount,
        textoCompletoLength: textoCompleto.length,
        stopReason,
        toolUseDataCount: toolUseData.length,
        toolUseDataNames: toolUseData.map(t => t.name),
        isStreamingArtifact,
        artifactContentLength: artifactContent?.length || 0
      });

      // ğŸ“Š LOG DETALHADO: Por que o modelo parou?
      if (stopReason === 'end_turn') {
        console.log(`âœ… [Stream] Modelo completou resposta naturalmente (end_turn)`);
      } else if (stopReason === 'max_tokens') {
        console.warn(`âš ï¸ [Stream] LIMITE DE TOKENS ATINGIDO! Resposta truncada.`);
        console.warn(`   Texto gerado: ${textoCompleto.length} chars (~${Math.round(textoCompleto.length / 4)} tokens)`);
        console.warn(`   maxTokens configurado: ${maxTokens}`);
      } else if (stopReason === 'tool_use') {
        console.log(`ğŸ”§ [Stream] Modelo solicitou uso de ferramenta`);
      } else if (stopReason === 'stop_sequence') {
        console.log(`ğŸ›‘ [Stream] Modelo encontrou stop sequence`);
      } else {
        console.warn(`â“ [Stream] Motivo de parada desconhecido: ${stopReason}`);
      }

      // ğŸ¨ ARTIFACT: Se estava fazendo streaming de artifact, enviar evento final
      if (isStreamingArtifact && artifactMetadata) {
        console.log(`ğŸ¨ [Artifact Complete] Enviando artifact completo: ${artifactMetadata.title}`);
        console.log(`   ğŸ“Š EstatÃ­sticas do artifact:`);
        console.log(`      - ConteÃºdo: ${artifactContent.length} chars (~${Math.round(artifactContent.length / 4)} tokens)`);
        console.log(`      - Linhas: ${artifactContent.split('\n').length}`);
        console.log(`      - Stop Reason: ${stopReason}`);

        // âš ï¸ AVISO: Se conteÃºdo foi truncado
        if (stopReason === 'max_tokens') {
          console.warn(`   âš ï¸ ATENÃ‡ÃƒO: Artifact pode estar INCOMPLETO (limite de tokens atingido)`);
          // Adicionar aviso no final do conteÃºdo
          artifactContent += '\n\n---\n\nâš ï¸ **AVISO:** Este documento pode estar incompleto devido ao limite de tokens. Para documentos muito extensos, considere dividir em mÃºltiplas anÃ¡lises.';
        }

        try {
          onChunk({
            __artifact_complete: {
              ...artifactMetadata,
              content: artifactContent,
              stopReason // Incluir stopReason para debugging
            }
          });
          console.log(`   âœ… artifact_complete enviado (${artifactContent.length} chars)`);
        } catch (err) {
          console.error('[Artifact Complete] Erro ao enviar:', err.message);
        }
      }

      // âœ… CORREÃ‡ÃƒO: Se tem tool_use pendente, processar mesmo se stopReason != tool_use
      // Isso acontece quando o histÃ³rico Ã© muito longo e atinge max_tokens no meio do tool_use
      if (toolUseData.length > 0 && loopCount < MAX_TOOL_LOOPS - 1) {
        console.log(`ğŸ”§ [Stream] Tool use detected (${toolUseData.length} tools), processing even though stopReason=${stopReason}`);
        // Continuar para processar tools (nÃ£o fazer return aqui)
      } else if (stopReason !== 'tool_use' || toolUseData.length === 0) {
        // Sem tool_use OU loops esgotados - retornar resposta final
        console.log(`âœ… [Stream] Returning final response (no tool use or max loops). Length: ${textoCompleto.length}`);

        if (textoCompleto.length === 0) {
          console.error(`âŒ [Stream] CRITICAL: textoCompleto is EMPTY! No text was generated by Bedrock.`);
          console.error(`   stopReason: ${stopReason}`);
          console.error(`   eventCount: ${eventCount}`);
          console.error(`   toolUseData: ${toolUseData.length}`);
          console.error(`   currentMessagesLength: ${currentMessages.length}`);
          console.error(`   lastMessage: ${JSON.stringify(currentMessages[currentMessages.length - 1])?.substring(0, 500)}`);
          console.error(`   ğŸ” This usually means:`);
          console.error(`      1. Bedrock API returned no contentBlockDelta events with text`);
          console.error(`      2. System prompt may be missing or malformed`);
          console.error(`      3. Request may have been rejected by Bedrock`);
          console.error(`      4. Model may be refusing to respond to this specific request`);
          console.error(`      5. History is too long and hits max_tokens before generating text (MOST COMMON)`);

          // ğŸ”¥ FALLBACK: Se histÃ³rico muito longo, tentar com histÃ³rico truncado agressivamente
          if (stopReason === 'max_tokens' && currentMessages.length > 3) {
            console.warn(`ğŸ”„ [Stream] Retrying with aggressively truncated history (keeping only last 2 messages)...`);

            // Manter apenas Ãºltimas 2 mensagens + system prompt
            const lastTwoMessages = currentMessages.slice(-2);

            return conversarStream(prompt, onChunk, {
              modelo,
              systemPrompt,
              historico: [], // Limpar histÃ³rico completamente
              kbContext: '',
              maxTokens,
              temperature,
              enableTools
            });
          }
        }

        return {
          sucesso: true,
          resposta: textoCompleto,
          modelo
        };
      }

      // âœ… Executar ferramentas solicitadas
      console.log(`ğŸ”§ [Stream] Tool use detected: ${toolUseData.map(t => t.name).join(', ')}`);

      // âš¡ CRÃTICO: Enviar feedback IMEDIATO para o usuÃ¡rio nÃ£o ficar esperando
      const toolNames = toolUseData.map(t => {
        if (t.name === 'pesquisar_jurisprudencia') return 'ğŸ” Buscando jurisprudÃªncia';
        if (t.name === 'pesquisar_jusbrasil') return 'ğŸ“š Consultando JusBrasil';
        if (t.name === 'consultar_cnj_datajud') return 'ğŸ›ï¸ Consultando CNJ DataJud';
        if (t.name === 'pesquisar_sumulas') return 'ğŸ“‹ Buscando sÃºmulas';
        if (t.name === 'consultar_kb') return 'ğŸ’¾ Consultando base de conhecimento';
        if (t.name === 'pesquisar_doutrina') return 'ğŸ“š Buscando doutrina jurÃ­dica';
        return `âš™ï¸ ${t.name}`;
      }).join(', ');

      onChunk(`\n\n${toolNames}...\n\n`);

      // Adicionar mensagem do assistente com tool_use
      const assistantMessage = {
        role: 'assistant',
        content: toolUseData.map(t => ({
          toolUse: {
            toolUseId: t.toolUseId,
            name: t.name,
            input: t.input
          }
        }))
      };
      currentMessages.push(assistantMessage);

      // Executar cada ferramenta e adicionar resultados
      const toolResults = [];
      let previewShown = false;

      for (const tool of toolUseData) {
        console.log(`ğŸ”§ Executando ferramenta: ${tool.name}`);
        console.log(`   ğŸ“‹ Tool Input:`, JSON.stringify(tool.input, null, 2)); // ğŸ†• LOG DO INPUT

        // âš¡ FEEDBACK: Informar ao usuÃ¡rio que a ferramenta estÃ¡ sendo executada
        const toolStartMsg = tool.name === 'pesquisar_jurisprudencia' ? 'â³ Consultando tribunais...' :
                            tool.name === 'pesquisar_jusbrasil' ? 'â³ Acessando JusBrasil...' :
                            tool.name === 'consultar_cnj_datajud' ? 'â³ Acessando DataJud...' :
                            tool.name === 'pesquisar_sumulas' ? 'â³ Buscando sÃºmulas...' :
                            tool.name === 'consultar_kb' ? 'â³ Consultando documentos...' :
                            `â³ Executando ${tool.name}...`;
        onChunk(toolStartMsg);

        try {
          const result = await executeTool(tool.name, tool.input);

          // ğŸ¨ ESPECIAL: Se for create_artifact, enviar evento especial para o frontend
          if (tool.name === 'create_artifact' && result.success && result.artifact) {
            console.log(`ğŸ“„ [Stream] Artifact detectado: "${result.artifact.title}"`);

            // Enviar artifact como objeto especial (nÃ£o string)
            // O chat-stream.js vai detectar e enviar como evento SSE tipo "artifact"
            try {
              onChunk({ __artifact: result.artifact });
            } catch (err) {
              console.error('[Bedrock Stream] Erro ao enviar artifact:', err.message);
            }
          }

          // âš¡ FEEDBACK: Informar resultado da ferramenta
          const successMsg = result.success ? ' âœ“\n' : ' âœ—\n';
          onChunk(successMsg);

          // âš¡ DETECTAR se encontrou jurisprudÃªncia - para forÃ§ar apresentaÃ§Ã£o imediata
          if (result.success && (tool.name === 'pesquisar_jurisprudencia' || tool.name === 'pesquisar_sumulas' || tool.name === 'pesquisar_doutrina')) {
            // Verificar se tem resultados reais (nÃ£o vazio)
            const hasResults = result.content && (
              result.content.includes('**[1]') || // Formato de resultado
              result.content.includes('Resultados:') ||
              result.content.length > 500 // Content substancial
            );
            if (hasResults) {
              hasJurisprudenceResults = true;
              console.log(`âœ… [Stream] JurisprudÃªncia encontrada em ${tool.name} - apresentaÃ§Ã£o serÃ¡ forÃ§ada`);
            }
          }

          // âš¡ PREVIEW IMEDIATO: Mostrar primeiros resultados assim que chegam (anti-silÃªncio)
          if (!previewShown && result.success && result.content && tool.name === 'pesquisar_jurisprudencia') {
            const previewMatch = result.content.match(/\*\*\[1\]\s+(.{0,150})/);
            if (previewMatch) {
              onChunk(`\nğŸ’¡ Preview: ${previewMatch[1]}...\n`);
              previewShown = true;
            }
          }

          toolResults.push({
            toolResult: {
              toolUseId: tool.toolUseId,
              content: [{
                text: result.success ? result.content : `Erro: ${result.error || result.content}`
              }]
            }
          });
          console.log(`âœ… Ferramenta ${tool.name} executada com sucesso`);
        } catch (error) {
          console.error(`âŒ Erro ao executar ${tool.name}:`, error);
          onChunk(' âœ— (erro)\n');
          toolResults.push({
            toolResult: {
              toolUseId: tool.toolUseId,
              content: [{
                text: `Erro ao executar ferramenta: ${error.message}`
              }]
            }
          });
        }
      }

      // Adicionar resultados das ferramentas Ã s mensagens
      currentMessages.push({
        role: 'user',
        content: toolResults
      });

      // âš¡ STREAMING FORÃ‡ADO: Enviar header para forÃ§ar Claude a comeÃ§ar a escrever
      onChunk(`âœ… Pesquisa concluÃ­da.\n\nğŸ“Š **Resultados Encontrados:**\n\n`);

      loopCount++;

      // ğŸš¨ v2.9.0: VELOCIDADE CRÃTICA - Com MAX_TOOL_LOOPS=2, forÃ§amos apresentaÃ§Ã£o apÃ³s 1 busca
      // Loop 0 -> busca -> loopCount++ = 1 -> shouldForcePresentation = TRUE (1 >= 2-1)
      // Isso elimina 75% da latÃªncia SSE (de 24-30s para 6-8s)
      const shouldForcePresentation = hasJurisprudenceResults || loopCount >= (MAX_TOOL_LOOPS - 1);

      if (shouldForcePresentation) {
        const reason = hasJurisprudenceResults ?
          `âœ… JurisprudÃªncia encontrada apÃ³s ${loopCount} loop(s) - APRESENTAÃ‡ÃƒO IMEDIATA para velocidade` :
          `âš ï¸ MAX_TOOL_LOOPS atingido (${loopCount}/${MAX_TOOL_LOOPS}) - FORÃ‡ANDO apresentaÃ§Ã£o`;
        console.log(`[Stream] ${reason}`);

        // v2.9.0: Registrar metricas de apresentacao forcada
        metrics.observeToolLoops(loopCount);
        metrics.incrementForcedPresentations();

        // Adicionar mensagem IMPERATIVA para forÃ§ar Claude a apresentar
        currentMessages.push({
          role: 'user',
          content: [{
            text: `ğŸš¨ IMPERATIVO CRÃTICO - APRESENTAÃ‡ÃƒO OBRIGATÃ“RIA

VocÃª executou ${loopCount} buscas de jurisprudÃªncia. As ferramentas retornaram resultados COMPLETOS nas mensagens acima.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AGORA vocÃª DEVE IMEDIATAMENTE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. APRESENTAR TODOS os resultados encontrados (sÃºmulas, decisÃµes, temas, IRDR, teses jurisprudenciais, acÃ³rdÃ£os, doutrina)

2. Para CADA resultado encontrado nas ferramentas acima, escreva:
   ğŸ“‹ **[NÃºmero] TÃ­tulo/Ementa**
   Tribunal: [tribunal]
   Data: [data se disponÃ­vel]
   Tipo: [sÃºmula/decisÃ£o/tese/IRDR/doutrina]
   Ementa: [resumo da ementa - MÃNIMO 2 linhas]
   Link: [URL completo]

3. ORGANIZE por relevÃ¢ncia e tipo

4. ApÃ³s listar TODOS os resultados, faÃ§a uma ANÃLISE JURÃDICA respondendo Ã  pergunta do usuÃ¡rio com base nos resultados

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PROIBIÃ‡Ã•ES ABSOLUTAS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âŒ NÃƒO execute mais buscas
âŒ NÃƒO diga "nÃ£o encontrei resultados" (vocÃª JÃ encontrou!)
âŒ NÃƒO resuma em 1 linha (detalhe CADA resultado)
âŒ NÃƒO omita nenhum resultado encontrado

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COMECE AGORA escrevendo "Com base nas buscas realizadas, encontrei:" e LISTE IMEDIATAMENTE o primeiro resultado!`
          }]
        });

        // Executar UMA Ãºltima iteraÃ§Ã£o APENAS para apresentaÃ§Ã£o
        // âš ï¸ IMPORTANTE: Manter MESMAS tools (nÃ£o remover) pois mensagens anteriores tÃªm toolUse blocks
        // A mensagem imperativa do user vai PROIBIR Claude de usar tools, mesmo que estejam disponÃ­veis
        const finalCommand = new ConverseStreamCommand({
          ...commandParams,
          messages: currentMessages
          // toolConfig mantÃ©m o mesmo de commandParams (com todas as tools)
        });
        const finalResponse = await retryAwsCommand(client, finalCommand, { modelId: commandParams.modelId, operation: 'converse_stream' });

        let finalText = '';
        for await (const event of finalResponse.stream) {
          if (event.contentBlockDelta?.delta?.text) {
            const chunk = event.contentBlockDelta.delta.text;
            finalText += chunk;
            onChunk(chunk);
          }
        }

        // v2.9.0: Registrar tempo total de streaming SSE
        const streamTotalTime = Date.now() - streamStartTime;
        metrics.observeSseStreamingTime(streamTotalTime);
        console.log(`ğŸ“Š [Metrics] SSE streaming completed in ${streamTotalTime}ms (loops: ${loopCount})`);

        return {
          sucesso: true,
          resposta: finalText,
          modelo,
          streamingTimeMs: streamTotalTime,
          loopsExecutados: loopCount
        };
      }
      // Loop continua para prÃ³xima iteraÃ§Ã£o
    }

    // Se chegou aqui sem stopReason, retornar erro
    console.error(`âŒ [Stream] Loop terminou sem resposta final`);
    return {
      sucesso: false,
      erro: 'Sistema atingiu limite de iteraÃ§Ãµes sem gerar resposta',
      modelo
    };
  } catch (error) {
    return {
      sucesso: false,
      erro: error.message,
      modelo
    };
  }
}

/**
 * Lista modelos disponÃ­veis na conta
 */
export async function listarModelos() {
  const client = getBedrockManagementClient();

  try {
    const command = new ListFoundationModelsCommand({});
    const response = await retryAwsCommand(client, command, { operation: 'list_foundation_models' });

    return response.modelSummaries.map(model => ({
      id: model.modelId,
      nome: model.modelName,
      provedor: model.providerName,
      modalidades: model.inputModalities,
      streaming: model.responseStreamingSupported
    }));
  } catch (error) {
    return { erro: error.message };
  }
}

/**
 * Lista inference profiles ativos
 */
export async function listarInferenceProfiles() {
  const client = getBedrockManagementClient();

  try {
    const command = new ListInferenceProfilesCommand({});
    const response = await retryAwsCommand(client, command, { operation: 'list_inference_profiles' });

    return response.inferenceProfileSummaries.map(profile => ({
      id: profile.inferenceProfileId,
      nome: profile.inferenceProfileName,
      status: profile.status,
      tipo: profile.type
    }));
  } catch (error) {
    return { erro: error.message };
  }
}

// ============================================================
// FUNÃ‡Ã•ES ESPECÃFICAS PARA ROM AGENT
// ============================================================

/**
 * Gera texto jurÃ­dico usando Bedrock
 * @param {string} tipo - Tipo de peÃ§a (peticao_inicial, habeas_corpus, etc)
 * @param {string} contexto - Contexto/fatos do caso
 * @param {object} options - OpÃ§Ãµes adicionais
 */
export async function gerarTextoJuridico(tipo, contexto, options = {}) {
  const systemPrompt = `VocÃª Ã© o ROM - Redator de Obras Magistrais, um assistente jurÃ­dico especializado em redaÃ§Ã£o de peÃ§as processuais brasileiras.

REGRAS OBRIGATÃ“RIAS:
- NUNCA use emojis
- NUNCA use markdown
- Use formataÃ§Ã£o profissional para documentos jurÃ­dicos
- Cite legislaÃ§Ã£o e jurisprudÃªncia quando aplicÃ¡vel
- Siga a estrutura tÃ©cnica adequada ao tipo de peÃ§a
- Use linguagem formal e tÃ©cnica do direito brasileiro`;

  const prompt = `Elabore uma ${tipo} com base no seguinte contexto:

${contexto}

Siga a estrutura tÃ©cnica adequada e inclua fundamentaÃ§Ã£o legal e jurisprudencial.`;

  return conversar(prompt, {
    ...options,
    systemPrompt,
    modelo: options.modelo || MODELOS_BEDROCK.amazon['nova-pro']
  });
}

/**
 * Analisa processo judicial
 */
export async function analisarProcesso(documentos, options = {}) {
  const systemPrompt = `VocÃª Ã© um analista jurÃ­dico especializado. Analise os documentos do processo e forneÃ§a:
1. Resumo dos fatos
2. Partes envolvidas
3. Pedidos/pretensÃµes
4. Fundamentos jurÃ­dicos
5. Pontos crÃ­ticos
6. SugestÃµes de estratÃ©gia`;

  return conversar(documentos, {
    ...options,
    systemPrompt,
    modelo: options.modelo || MODELOS_BEDROCK.amazon['nova-pro']
  });
}

/**
 * AnÃ¡lise jurÃ­dica profunda com DeepSeek R1 (modelo de raciocÃ­nio)
 * Ideal para: teses complexas, anÃ¡lise de precedentes, fundamentaÃ§Ã£o
 */
export async function analisarComRaciocinio(questao, options = {}) {
  const systemPrompt = `VocÃª Ã© um jurista brasileiro especializado em anÃ¡lise jurÃ­dica profunda.
Analise a questÃ£o apresentada com raciocÃ­nio detalhado, considerando:
1. LegislaÃ§Ã£o aplicÃ¡vel (CF, cÃ³digos, leis especiais)
2. JurisprudÃªncia relevante (STF, STJ, tribunais estaduais)
3. Doutrina majoritÃ¡ria
4. Argumentos favorÃ¡veis e contrÃ¡rios
5. ConclusÃ£o fundamentada

Seja preciso nas citaÃ§Ãµes legais e jurisprudenciais.`;

  const resultado = await conversar(questao, {
    ...options,
    systemPrompt,
    modelo: 'deepseek.r1-v1:0',
    maxTokens: options.maxTokens || 2000
  });

  return {
    ...resultado,
    tipo: 'analise_raciocinio',
    modelo: 'DeepSeek R1'
  };
}

/**
 * Pesquisa jurisprudÃªncia
 */
export async function pesquisarJurisprudencia(tema, options = {}) {
  const systemPrompt = `VocÃª Ã© um pesquisador jurÃ­dico especializado em jurisprudÃªncia brasileira.
ForneÃ§a precedentes relevantes sobre o tema, indicando:
- Tribunal
- NÃºmero do processo
- Relator
- Data do julgamento
- Tese firmada

IMPORTANTE: Indique que os precedentes devem ser verificados nas fontes oficiais.`;

  return conversar(`Pesquise jurisprudÃªncia sobre: ${tema}`, {
    ...options,
    systemPrompt,
    modelo: options.modelo || MODELOS_BEDROCK.amazon['nova-pro']
  });
}

// ============================================================
// CLASSE PRINCIPAL
// ============================================================

export class BedrockAgent {
  constructor(options = {}) {
    this.modelo = options.modelo || CONFIG.defaultModel;
    this.region = options.region || CONFIG.region;
    this.systemPrompt = options.systemPrompt || null;
    this.historico = [];

// Debug logging removido para producao - agente inicializado silenciosamente
  }

  async enviar(mensagem, options = {}) {
    // ğŸ”¥ NÃƒO concatenar aqui - deixar conversar() fazer isso DEPOIS do truncamento
    const { kbContext, ...restOptions } = options;

    const resultado = await conversar(mensagem, {
      modelo: this.modelo,
      systemPrompt: this.systemPrompt,
      historico: this.historico,
      kbContext: kbContext || '',  // Passar para truncamento correto
      ...restOptions
    });

    if (resultado.sucesso) {
      // Salvar no histÃ³rico a mensagem ORIGINAL (sem KB) para economizar espaÃ§o
      this.historico.push({ role: 'user', content: mensagem });
      this.historico.push({ role: 'assistant', content: resultado.resposta });
    }

    return resultado;
  }

  async enviarStream(mensagem, onChunk, options = {}) {
    return conversarStream(mensagem, onChunk, {
      modelo: this.modelo,
      systemPrompt: this.systemPrompt,
      historico: this.historico,
      ...options
    });
  }

  limparHistorico() {
    this.historico = [];
  }

  setModelo(modelo) {
    this.modelo = modelo;
  }

  setSystemPrompt(prompt) {
    this.systemPrompt = prompt;
  }
}

// ============================================================
// EXPORTS
// ============================================================

export default {
  // ConfiguraÃ§Ã£o
  CONFIG,
  MODELOS_BEDROCK,
  INFERENCE_PROFILES,

  // FunÃ§Ãµes principais
  conversar,
  conversarStream,
  listarModelos,
  listarInferenceProfiles,

  // FunÃ§Ãµes ROM Agent
  gerarTextoJuridico,
  analisarProcesso,
  pesquisarJurisprudencia,
  analisarComRaciocinio, // DeepSeek R1

  // Classe
  BedrockAgent
};
